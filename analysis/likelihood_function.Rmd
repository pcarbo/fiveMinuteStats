---
title: The likelihood function
author: 
  name: Matthew Stephens
  affiliation: University of Chicago
date: January 12, 2026
output:
  pdf_document:
    keep_tex: false
    latex_engine: pdflatex
    template: readable.tex
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
spacing: single
graphics: yes
endnote: no
---

See [here][pdf_version] for a PDF version of this vignette.

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,fig.align = "center")
```

Prerequisites
=============

You should understand the concept of using likelihood ratio for
[discrete data][likelihood_ratio_simple_models] and [continuous
data][likelihood_ratio_simple_continuous_data] to compare support
for two fully specified models.

Overview
========

We have seen how one can use the likelihood ratio to compare the
support in the data for two fully-specified models. In practice, we
often want to compare more than two models â€” indeed, we often want to
compare a continuum of models. This is where the idea of a *likelihood
function* comes from.

Example
=======

In the example [here][likelihood_ratio_simple_models], we assumed that
the frequencies of different alleles (genetic types) in forest and
savanna elephants were given to us. In practice, these frequencies
themselves would have to be estimated from data.

For example, suppose we collect data on 100 savanna elephants, and we
see that 30 of them carry allele 1 at marker 1, and 70 carry the
allele 0 at marker 1 (again we are treating elephants as haploid to
simplify things). Intuitively, we might estimate that the frequency of
the 1 allele at that marker is 30/100, or 0.3. But we might think that
the data are also consistent with other frequencies near 0.3. For
example, maybe the data are also consistent with a frequency of 0.29.
Or 0.28? Or...

In this case, we have many more than just two models to
compare. Indeed, if we allow that the frequency could, in principle,
lie anywhere in the interval [0,1], then we have a continuum of models
to compare.

Specifically, for each $q\in [0,1]$, let $M_q$ denote the model that
the true frequency of the 1 allele is $q$. Then, given our observation
that 30 of 100 elephants carry allele 1 at marker 1, the likelihood
for model $M_q$ is, by the previous definition,
$$
L(M_q) = \Pr(D \mid M_q) = q^{30} (1-q)^{70}.
$$
And the LR comparing models $M_{q_1}$ and $M_{q_2}$ is
$$
\mathrm{LR}(M_{q_1}, M_{q_2}) = [q_1/q_2]^{30} [(1-q_1)/(1-q_2)]^{70}.
$$

This is an example of what is called a *parametric model.* A parametric
model is collection of models indexed by a parameter vector, often
denoted $\theta$, whose values lie in some *parameter space*, usually
denoted $\Theta$. The number of parameters included in the vector
$\theta$ is called the "dimensionality" of the model or parameter
space, and often denoted $\mathrm{dim}(\Theta)$.

Here the parameter is $q$, and the parameter space is $[0,1]$. The
dimensionality of the parameter space is 1.

When computing likelihoods for parametric models, we usually dispense
with the model notation and simply use the parameter value to denote
the model. So instead of referring to the likelihood for $M_q$ we just
say the "likelihood for $q$", and write $L(q)$. So the likelihood for
$q$ is given by
$$
L(q) = q^{30} (1-q)^{70}.
$$
Correspondingly, we can also refer to the "likelihood ratio for $q_1$
vs. $q_2$".

We could plot the likelihood function as follows:

```{r likelihood, fig.height=3.5, fig.width=4}
q <- seq(0,1,length.out = 100)
L <- function (q)
  q^30 * (1-q)^70
plot(q,L(q),ylab = "L(q)",xlab = "q",type = "l")
```

The value of $\theta$ that maximizes the likelihood function is
referred to as the "maximum likelihood estimate", and usually denoted
$\hat{\theta}$. That is,
$$
\hat{\theta} := \underset{\theta \,\in\, \Theta}{\mathrm{argmax}} \, L(\theta).
$$

Provided the data are sufficiently informative and the number of
parameters is not too large, maximum likelihood estimates tend to be
sensible. In this case, we can see that the maximum likelihood estimate
is $q = 0.3$, which corresponds to our intuition.

Note that from the likelihood function we can easily compute the
likelihood ratio for any pair of parameter values! And just as with
comparing two models, it is not the likelihoods that matter, but the
likelihood ratios. That is, you can divide the likelihood function by
any constant without affecting the likelihood ratios.

One way to emphasize this is to standardize the likelihood function so
that its maximum is at 1, by dividing $L(\theta)/L(\hat{\theta})$.

```{r likelihood-ratio, fig.height=3.5, fig.width=4}
q <- seq(0,1,length.out = 100)
L <- function (q)
  q^30 * (1-q)^70
plot(q,L(q)/L(0.3),ylab = "L(q)/L(qhat)",xlab = "q",type = "l")
```

Note that, for some values of $q$, the likelihood ratio compared with
$q=0.3$ is very close to zero. These values of $q$ are so much less
consistent with the data that they are effectively excluded by the
data.  Just looking at the picture we might say that the values of $q$
less than 0.15 or bigger than 0.5 are pretty much excluded by the
data.  We will see later how Bayesian analysis methods can be used to
make this kind of argument more formal.

The log-likelihood
==================

Just as it can often be convenient to work with the log-likelihood
ratio, it can be convenient to work with the log-likelihood function,
usually denoted $l(\theta)$ ("lowercase L"). As with log-likelihood
ratios, unless otherwise specified, we use the "base e" log. Here is
the log-likelihood function:

```{r log-likelihood, fig.height=3.5, fig.width=4}
q <- seq(0,1,length.out = 100)
l <- function (q)
  30*log(q) + 70 * log(1-q)
plot(q,l(q) - l(0.3),ylab = "l(q) - l(qhat)",xlab = "q",type = "l",
     ylim = c(-10,0))
```

Changes in the log-likelihood function are referred to as
"log-likelihood units". For example, the difference in the support for
$q = 0.3$ and $q = 0.35$ is $l(0.3) - l(0.35)$ which is `r l(0.3) - l(0.35)`
log-likelihood units. Again, remember that it is *differences* in $l$
that matter.

Notice that the scale of the Y axis in this plot was set to span 10
log-likelihood units. Setting the scale in this way makes sure the
plot focuses on the parts of the parameter space that have more than
minuscule support from the data (in this case, LR no smaller than
$1/e^{10}$. Without this, the plot can be much harder to read. For
example, here is the plot using the default scale selected by R:

```{r log-likelihood-no-limits, fig.height=3.5, fig.width=4}
plot(q,l(q) - l(0.3),ylab = "l(q) - l(qhat)",xlab = "q",type = "l")
```

Notice how different this plot looks to the eye even though it is
exactly the same curve being plotted (just different Y axis
scale). It is worth thinking about what scale you use when plotting
log-likelihoods (and, of course, figures in general!).

[pdf_version]: https://github.com/pcarbo/fiveMinuteStats/blob/master/docs/likelihood_function.pdf
[likelihood_ratio_simple_models]: https://pcarbo.github.io/fiveMinuteStats/likelihood_ratio_simple_models.html
[likelihood_ratio_simple_continuous_data]: https://pcarbo.github.io/fiveMinuteStats/likelihood_ratio_continuous_data.html
