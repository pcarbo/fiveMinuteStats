---
title: Gibbs sampling for clustering genetic data
author: 
  name: Matthew Stephens and Peter Carbonetto
  affiliation: University of Chicago
date: February 9, 2026
output:
  pdf_document:
    keep_tex: false
    latex_engine: pdflatex
    template: readable.tex
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
spacing: single
graphics: yes
endnote: no
---

See [here][pdf_version] for a PDF version of this vignette.

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,fig.align = "center")
```

Prerequisites
=============

Be familiar with [Bayesian inference for the two class
problem][LR_and_BF] and [conjugate Bayesian analysis for a
binomial proportion][bayes_conjugate].

Overview
========

Suppose we observe genetic data on a sample of $n$ elephants at $J$
genetic markers (or "loci") in the genome. For simplicity, we will
assume the elephants are haploid; that is, they have just one copy of
their genome. And we will assume that there are just two genetic types
("alleles") at each locus, which we will label as 0 and 1.

We will further assume that there are two type of elephant: forest
elephants and savanna elephants, and that the allele frequencies in
forest elephants are different from those in savanna elephants, but
that the allele frequencies for each of these two groups are unknown.
Also, we do not know which samples are forest elephants and which are
savanna elehants. Our goal is to infer both these sets of unknowns:
(i) which elephants are forest and which are savanna; (ii) what are
the allele frequencies in each group of elephants.

Notation 
========

Let $x_i$ denote the genetic data for individual $i$ ($i = 1,\dots,
n$). Thus, $x_i$ is a binary vector (a vector of zeros and ones) of
length $J$. Let $X$ denote the combined genetic data, a $n \times J$
matrix.

Let $z_i \in \{1, 2\}$ denote the group (forest vs. savanna) of
individual $i$, and let $z$ denote the vector $z = (z_1, \dots, z_n)$.

Let $f_{jk}$ denote the frequency of the 1 allele at locus $j$ in
group $k$ ($j = 1, \dots, J$, $k = 1, 2$). (Here, 1 = forest
and 2 = savanna.) Let $f_k$ denote the vector $(f_{1k},
\dots, f_{Jk})$, and let $F$ denote all the (unknown) allele frequencies
$F = (f_1, f_2)$. (So $F$ is a $J \times 2$ matrix.)

With this notation in place, we can state the problem, which is to
infer the unknowns $F$ and $z$ from the genetic data $X$.

Model
=====

To perform Bayesian inference for $z$ and $F$, we need to specify the
likelihood, $p(X \mid z, F)$, and a prior distribution, $p(z, F)$.

Likelihood
----------

For each individual, we will assume that if we knew its group of
origin, and we knew the allele frequencies in each group, then the
genetic data at the different markers are independent draws from the
relevant allele frequencies. This is exactly the model assumed
[here][likelihood_ratio_simple_models]. In mathematical notation, we
assume
$$
p(x_i \mid z_i, F) = p(x_i \mid z_i, f_{j1}, f_{j2}) =
\prod_{j=1}^J f_{j{z_i}}^{x_{ij}}(1 - f_{j{z_i}})^{1 - x_{ij}}.
$$
All the subscripts here make this difficult to read. 
To make things easier to read, we can replace $z_i$ with $k$:
$$
p(x_i \mid z_i = k, F) = p(x_i \mid z_i, f_{j1}, f_{j2}) =
\prod_{j=1}^J f_{jk}^{x_{ij}}(1 - f_{jk})^{1 - x_{ij}}.
$$

We will further assume that the different individuals (elephants) are
independent:
$$
p(X \mid z, F) = \prod_{i=1}^n p(x_i \mid z_i, F).
$$

This completes specification of the likelihood.

Prior
-----

We will assume that $F$ and $z$ are *a priori* independent, so $p(F, z)
= p(F) \, p(z)$.  This assumption seems reasonable: before seeing the
genetic data ($X$), telling you the allele frequencies in the two
groups would not tell you anything about the group membership of the
elephants. (Of course, after seeing the genetic data $X$, the allele
frequencies would help classify the individuals, so $F$ and $z$ are
not going to be *a posteriori* independent. However, here we are
concerned with the prior, not the posterior.)

For the prior on $F$, we will further assume that the allele
frequencies in each group at each locus are independent, so
$$
p(F) = \prod_{k=1}^2 \prod_{j=1}^J p(f_{jk}).
$$
This assumption could be
improved, but at the cost of considerable extra complexity, and so we
stick with independence for now. Also, for simplicity we will assume a
uniform prior distribution for $f_{jk}$, so $p(f_{jk}) = 1$.

For $z$, we will assume that the origin of each individual is
independent, and that the probability of arising from group $k$ is $q_k$.
So
$$
p(z) = \prod_{i=1}^n p(z_i),
$$
and $p(z_i = k) = q_k$, in which $q_1$, $q_2$ are taken to be known
(that is, we will not estimate them from the data). Again, this
assumption could be improved, but we start here for simplicity.

Computation
===========

Our goal is to compute (or sample from) the posterior distribution
$p(z, F \mid X)$, which by Bayes Theorem is
$$
p(z, F \mid X) \propto p(X \mid z, F) \, p(z, F).
$$

One way to sample from this distribution is to implement a Gibbs
sampler for $z, F$. This requires us to be able to do two things:

1. Sample from $p(z \mid F, X)$.

2. Sample from $p(F \mid z, X)$.

These are called the "full conditional distributions", or "full
conditions", for $z$ and $F$, respectively. The use of the word "full"
here indicates that they are conditional on *everything else* (the
data and all the other parameters).

Another way to describe this Gibbs sampler
-------------------------------------------

Because of the conditional independence assumptions we made above, the
Gibbs sampler we just described also can be described in the following way:

1. For $i = 1, \ldots, n$, sample from $p(z_i \mid F, X)$.

2. For $j = 1, \ldots, J$ and $k = 1, 2$, sample from
   $p(f_{jk} \mid z, X)$.

This algorithm is equivalent to the algorithm above.

In other words, because of the conditional independence assumptions we
made, it turns out that sampling $z$ given $(F, X)$ is the same as
sampling the group $z_i$ for a single elephant, one at a time.
Similarly, because of the conditional independence assumptions we
made, it turns out that sampling $F$ given $(z, X)$ is the same as
sampling an allele frequency $f_{jk}$ for a single locus and a single
group one at a time. This makes the Gibbs sampler particularly
easy to implement.

We will verify these two statements next.

Full conditional for z
----------------------

From the modeling assumptions made above, we know that
$$
\begin{aligned}
p(z \mid F, X) &\propto p(X \mid z, F) \, p(z) \, p(F) \\
&= \prod_{i=1}^n \{ p(x_i \mid z_i, F) \, p(z_i) \} \, p(F).
\end{aligned}
$$

So we see that the full conditional for $z = (z_1, \dots, z_n)$
factorizes over $i$ into terms that depend only on $z_i$ and not the
other elements of $z$; that is,
$$
p(z \mid F, X) \propto \prod_{i=1}^n \phi_i(z_i),
$$
in which the functions $\phi_i(z_i)$ are
$$
\phi_i(z_i) = p(x_i \mid z_i, F) \, p(z_i).
$$

This implies that the $z_i$ are *conditionally independent* given
$X, F$, which is very convenient as it means we can compute their
conditional distribution just by computing the marginals:
$$
p(z_i = k \mid F, X) \propto p(x_i \mid z_i = k, F) \, p(z_i = k).
$$
The exact expression for $p(z_i = k \mid F, X)$ is derived below.

Full conditional for F
----------------------

From the modeling assumptions made above, we know that
$$
p(F \mid z, X) \propto p(X \mid z, F) \, p(F) \, p(z) =
p(z) \, 
\prod_{j=1}^J \prod_{k=1}^2 p(f_{jk}) \times \big\{
{\textstyle \prod_{i=1}^n p(x_{ij} \mid z_i, f_{j1}, f_{j2})} \big\}.
$$
So we see that the full conditional for $F$ factorizes over $j$ and
$k$ into terms that depend only on $f_{jk}$ and not any of the other
allele frequencies. In other words,
$$
p(F \mid z, X) \propto \prod_{j=1}^J \prod_{k=1}^2
\psi_{jk}(f_{jk}),
$$
in which the functions $\psi_{jk}(f_{jk})$ are
$$
\psi_{jk}(f_{jk}) = p(f_{jk}) 
\prod_{i=1}^n p(x_{ij} \mid z_i, f_{j1}, f_{j2}).
$$
The exact expression for $p(f_{jk} \mid z, X)$ is derived below.

Simulate some data
==================

To illustrate, let's simulate data from this model:

```{r sim-data}
set.seed(33)
#' @param n The number of samples to simulate.
#' @param P A 2 x R matrix of allele frequencies.
r_simplemix <- function (n, P) {
  R <- ncol(P)
  z <- sample(2,prob = c(0.5,0.5),size = n,replace = TRUE)
  x <- matrix(0,n,R)
  for (i in 1:n)
    x[i,] <- rbinom(R,rep(1,R),P[z[i],])
  return(list(x = x,z = z))
}
P <- rbind(c(0.500,0.500,0.500,0.500,0.500,0.500),
           c(0.001,0.999,0.001,0.999,0.001,0.999))
sim <- r_simplemix(n = 50,P)
x <- sim$x
```

Gibbs sampler code
==================

```{r gibbs-sampler}
normalize <- function (x)
  x/sum(x)

#' @param x Data vector (length R).
#' @param P 2 x R matrix of allele frequencies.
#' @return The log-likelihood for each of the K populations.
log_pr_x_given_P <- function (x, P)
  colSums(x*log(t(P)) + (1-x)*log(1-t(P)))

#' @param x n x R data matrix.
#' @param P 2 x R matrix of allele frequencies.
#' @return Group memberships (vector of length n).
sample_z <- function (x, P) {
  K <- nrow(P)
  loglik_matrix <- apply(x,1,log_pr_x_given_P,P = P)
  lik_matrix <- exp(loglik_matrix) 
  p.z.given.x <- apply(lik_matrix,2,normalize)
  z <- rep(0,nrow(x))
  for (i in 1:length(z))
    z[i] <- sample(K,size = 1,prob = p.z.given.x[,i],replace = TRUE)
  return(z)
}

#' @param x n x R data matrix.
#' @param z Cluster allocations (vector of length n).
#' @return 2 x R matrix of allele frequencies.
sample_P <- function (x, z) {
  R <- ncol(x)
  P <- matrix(0,2,R)
  for (i in 1:2) {
    sample_size <- sum(z == i)
    if (sample_size == 0)
      number_of_ones <- rep(0,R)
    else
      number_of_ones <- colSums(x[z == i,])
    P[i,] <- rbeta(R,number_of_ones + 1,sample_size - number_of_ones + 1)
  }
  return(P)
}

gibbs <- function (x, niter = 100) {
  n <- nrow(x)
  z <- sample(2,n,replace = TRUE)
  out <- matrix(0,niter,n)
  out[1,] <- z 
  for(i in 2:niter) {
    P <- sample_P(x,z)
    z <- sample_z(x,P)
    out[i,] <- z
  }
  return(out)
}
```
  
Try the Gibbs sampler on the data simulated above:

```{r run-gibbs-sampler, fig.height=6, fig.width=4}
z <- gibbs(x,niter = 100)
table(mcmc = z[1,],true = sim$z)
table(mcmc = z[100,],true = sim$z)
image(t(z[100:1,]),xlab = "elephant",
      ylab = "Gibbs sampling iteration",
      col = c("red","darkblue"))
```

Additional derivations
======================

Full conditional for z
----------------------

Given that the full condition for $z$ factorizes over the individual
elephants $i$, without loss of generality we can derive the full
conditional assuming a data set with a single elephant. Dropping
the unneeded the $i$ subscripts, this is
$$
p(z = k \mid F, x) \propto p(x \mid z = k, F) \, p(z = k).
$$
Plugging in the expression for the likelihood and prior above, this is
$$
p(z = k \mid F, x) \propto
q_k \times \prod_{j=1}^J f_{jk}^{x_j}(1-f_{jk})^{1-x_j}.
$$
Note that an additional normalization step is needed to actually obtain
probabilities.

Full conditional for F
----------------------

TO DO

[pdf_version]: https://github.com/pcarbo/fiveMinuteStats/blob/master/docs/gibbs_structure_simple.pdf
[LR_and_BF]: https://pcarbo.github.io/fiveMinuteStats/LR_and_BF.html
[bayes_conjugate]: https://pcarbo.github.io/fiveMinuteStats/bayes_conjugate.html
[likelihood_ratio_simple_models]: https://pcarbo.github.io/fiveMinuteStats/likelihood_ratio_simple_models.html
