---
title: The Metropolis Hastings algorithm, Part 2
author: 
  name: Matthew Stephens
  affiliation: University of Chicago
date: January 23, 2026
output:
  pdf_document:
    keep_tex: false
    latex_engine: pdflatex
    template: readable.tex
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
spacing: single
graphics: yes
endnote: no
---

See [here][pdf_version] for a PDF version of this vignette.

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,fig.align = "center")
```

Prerequisites
=============

You should be familiar with the basics of the 
[Metropolis-Hastings algorithm][MH_intro].

Introduction 
============

In this vignette, we follow up on the original algorithm with a couple of
points: (i) numerical issues that were glossed over, and (ii) a useful plot.

Avoiding numerical issues in the acceptance probability
=======================================================

The key to the M-H algorithm is computing the acceptance probability,
which recall is given by
$$
A = \min \bigg\{ 1, \frac{\pi(y) \, Q(x_t \mid y)}{\pi(x_t) \, Q(y \mid x_t)}
\bigg\}.
$$
In practice, both terms in the fraction may be very close to zero, so on a
computer you should do this computation on the logarithmic scale
before exponentiating, something like this:
$$
\frac{\pi(y) \, Q(x_t \mid y)}{\pi(x_t) \, Q(y \mid x_t)} =
\exp\big\{
\log\pi(y) - \log \pi(x_t) + \log Q(x_t \mid y) - \log Q(y \mid x_t)
\big\}].
$$
Furthermore, the log values in this expression should be computed
directly, and not by computing them and then taking the log. For
example, in our previous example we sampled from a target distribution,
which was the exponential:
$$
\pi(x) = e^{-x}, \quad x > 0.
$$

Instead, we can directly compute $\log\pi(x) = -x$. The code we had
in that previous example can therefore be written as follows:

```{r metropolis}
log_target <- function (x) 
  ifelse(x < 0,-Inf,-x)
x <- rep(0,10000)
x[1] <- 100
for (i in 2:10000) {
  current_x <- x[i-1]
  proposed_x <- current_x + rnorm(1)
  A <- exp(log_target(proposed_x) - log_target(current_x))
  u <- runif(1)
  if (u < A)
    x[i] <- proposed_x
  else
    x[i] <- current_x
}
```

An important and useful plot
============================

In practice, MCMC is usually performed in high-dimensional space. It
can therefore be really hard to visualize directly the values of the
chain. A simple 1-d summary that is always available is the log-target
density, $\log \pi(x)$. So you should usually plot a trace of this
whenever you run an MCMC scheme:

```{r plot-log-target, fig.height=3, fig.width=5}
plot(log_target(x),xlab = "time",ylab = "log target",
     main = "log-target value")
```

Here the plot shows "typical" behaviour of MCMC scheme: because the
starting point was not close to the optimal of $\pi$, the
chain initially makes large changes to find a part of the space
where $\pi(x)$ is "large". Once it finds that part of the space, it
starts to explore around the region where $\pi(x)$ is large.

Burn-in
=======

The plot in the previous section immediately shows that there is an
initial period of time where the Markov chain is unduly influenced by
its starting position. In other words, during those iterations the
Markov chain has not "converged" and those samples should not be
considered to be samples from $\pi$. To address this, it is common to
discard the first set of iterations of any chain; the iterations that
are discarded are often called "burn-in".

Based on the plot, we might discard the first 1,000 iterations or so
as "burn-in". Here are comparisons of the samples with and without
burnin discarded:

```{r burnin, fig.height=3, fig.width=6}
par(mfrow = c(1,2))
hist(x,xlab = "x",main = "burnin not discarded")
hist(x[-(1:1000)],xlab = "x",main = "burnin discarded")
```

[pdf_version]: https://github.com/pcarbo/fiveMinuteStats/blob/master/docs/MH_intro_02.pdf
[MH_intro]: https://pcarbo.github.io/fiveMinuteStats/MH_intro.html
