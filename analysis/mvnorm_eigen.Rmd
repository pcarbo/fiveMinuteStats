---
title: Interpreting multivariate normal via eigen-decomposition of covariance matrix
author: 
  name: Matthew Stephens
  affiliation: University of Chicago
date: February 11, 2026
output:
  pdf_document:
    keep_tex: false
    latex_engine: pdflatex
    template: readable.tex
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
spacing: single
graphics: yes
endnote: no
---

See [here][pdf_version] for a PDF version of this vignette.

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,fig.align = "center")
```

## Prerequisites

You should be familiar with the
[multivariate normal distribution][mvnorm], and with the
eigen-decomposition for symmetric positive semi-definite (PSD)
matrices.


Introduction
============

Getting an intuition for what the $p$-dimensional multivariate normal
distribution $N_p(\mu, \Sigma)$ "looks like" can be difficult. For $p
= 1$ or 2, things are not too bad; we can directly visualize a
univariate normal distribution by plotting its density, and visualize
a bivariate normal distribution by plotting a contour plot of the
density, or by simulating samples from the distribution and
visualizing them using a 2-d scatterplot. For example, the following
code does this for $N(0, \Sigma)$, where
$$
\Sigma = \begin{pmatrix}
1.0 & 0.7 \\ 0.7 & 1.0
\end{pmatrix}:
$$

```{r plot-cov-2d, fig.height=3.5, fig.width=3}
library(mvtnorm)
Sigma <- cbind(c(1,0.7),c(0.7,1))
X <- rmvnorm(1000,c(0,0),Sigma)
plot(X[,1],X[,2],xlab = "x1",ylab = "x2",asp = 1)
```

But in $p = 100$ dimensions, or even just $p = 4$ dimensions, things
become much harder because direct visualization is impractical.
So how can we get intuition about the multivariate normal
distribution, $N_p(\mu, \Sigma)$ when $p$ is large?

Note first that the mean $\mu$ is just a vector of $p$ numbers, and
generally causes few problem in interpretation: you can just think of
each number as specifying the mean in each of the $p$ coordinates one
at a time.

In contrast, the covariance matrix $\Sigma$ is a $p \times p$ matrix
that captures potentially more complex patterns, and creates more
challenges for intuition. One possible approach is to plot a heatmap
of this matrix, and this can certainly be helpful in certain
situations. However, this vignette describes a more algebraic
approach based on the eigen-decomposition of $\Sigma$.

Some linear algebra
===================

Recall that any valid $p \times p$ covariance matrix $\Sigma$ must be
symmetric and positive semi-definite (PSD). Furthermore, recall that
any such PSD matrix must have eigen-decomposition:
$$
\Sigma = V \Lambda V',
$$ 
where

+ $\Lambda$ is a $K \times K$ diagonal matrix with the non-zero
  eigenvalues of $\Sigma$, $\lambda_1, \dots, \lambda_K$, on the
  diagonal ($K \leq p$ is the rank of $\Sigma$).

+ $V$ is a $p \times K$ orthonormal matrix ($V^TV = I_K$), whose columns
  $v_1, \dots, v_K$ are the normalized eigenvectors of $\Sigma$
  corresponding to the non-zero eigenvalues.
  
Recall also that if $Z \sim N_p(0, I_p)$, $A$ is any $n \times p$
matrix, and $X = \mu + AZ$, then $X \sim N(\mu, AA^T)$.

Now apply this last result with $A= V \Lambda^{1/2}$ where
$\Lambda^{1/2}$ is the diagonal matrix with
$\lambda_1^{1/2},\dots,\lambda_K^{1/2}$ on the diagonal. For $X =
\mu + AZ$, we get
$$
X \sim N_p(\mu, V \Lambda^{1/2} \Lambda^{1/2} V^T).
$$
That is,
$$
X \sim N_p(\mu, \Sigma).
$$

We can write the matrix multiple $V\Lambda^{1/2} Z$ as a sum to make
the structure more obvious:
$$
\mu + \sum_{k=1}^K \lambda_k^{1/2} z_k v_k  \sim N_p(\mu, \Sigma).
$$
Here, $\mu$ and $v_1, \dots, v_K$ are all column vectors of length $p$,
whereas the $\lambda_k$ and $z_k$ are all scalars.

Interpration as a random linear combination of eigenvectors
-----------------------------------------------------------

From this algebra, if $X \sim N_p(\mu,\Sigma)$, then we can think of
$X$ as being generated by taking the mean $\mu$, and adding a *random
linear combination* of the eigenvectors of $\Sigma$. Specifically,
$$
X = \mu + \sum_{k=1}^K b_k v_k,
$$ 
where the weights 
$$
b_k=\lambda_k^{1/2} z_k \sim N(0,\lambda_k).
$$
are independent of one another. 

Note that if $\lambda_k$ is "small" then $b_k \approx 0$, so the
eigenvectors with small eigenvalues contribute little to $X$, and we
can focus on the eigenvectors with large eigenvalues.  Indeed, this
approach provides the simplest insights when most of the $\lambda_k$
are negligible and only one or two eigenvectors contribute
meaningfully to the sum.

Example: rank-1 covariance
==========================

To make a simple example, set $\mu=0$ and assume $\Sigma$ is a rank 1
matrix. That is, $\Sigma$ has only one eigenvector:
$$
\Sigma = \lambda vv^T
$$
for some $p$-vector $v$.

In this case, the algebra above gives the representation $X= b v$
where $b \sim N(0,\lambda)$. That is $X$ is simply a multiple of $v$,
where the multiplier is randomly distributed from a univariate
normal. Thus in this case the randomness in $X$ boils down to the
randomness in a single random univarate normal, which is easy to
visualize.

To give a specific example, suppose that $v$ is the vector of all 1s
$v=(1,\dots,1)$ and $\lambda=1$. That is $\Sigma$ is a matrix of all
1s.  Then $X= (b,b,b,\dots,b)$ where $b \sim N(0,1)$.

To give another specific example, if $v=(-1,-1,-1,1,1)$ and
$\lambda=2$ then $X= (-b,-b,-b,b,b)$ where $b \sim N(0,2)$.

[pdf_version]: https://github.com/pcarbo/fiveMinuteStats/blob/master/docs/mvnorm_eigen.pdf
[mvnorm]: https://pcarbo.github.io/fiveMinuteStats/mvnorm.html
