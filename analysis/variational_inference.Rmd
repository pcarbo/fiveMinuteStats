---
title: "Variational inference: approximations, objectives, and algorithms"
author: Peter Carbonetto
date: 2024-04-26
output: workflowr::wflow_html
---

## Prerequisites

+ MCMC sampling (and specifically Gibbs sampling) will be used to
  illustrate some of the key ideas so you should be familiar with
  those.

+ Properties of multivariate Gaussians.

+ It is helpful if you are familiar with ridge regression, but this
  isn't essential since we will (re)introduce it here. See for example
  [Ryan Tibshirani's class notes][ryantibs-ridge].

## Introduction

*Add introduction here.*

## Ridge regression

We will use the ridge regression model as our running example to
illustrate the use of variational inference to perform posterior
inferences. Although variational inference methods aren't really
needed because the math works out very well for this model, its
convenient mathematical properties will be helpful for understanding
the variational approximations since we can compare the approximations
to the exact calculations.

Although you may have seen the ridge regression model elsewhere, it
hasn't been introduced in another of the other fiveMinuteStats
vignettes, so we briefly (re)introduce it here.

Ridge regression can be introduced in a variety of ways. Here we will
view it as a Bayesian model; that is, we define a likelihood and a
prior, and our interest is in performing posterior inferences with
respect to this likelihood and prior.

We start with a standard linear regression model:
$$
y_i \sim N(\mathbf{x}_i^T\mathbf{b}, \sigma^2), \quad i = 1, \ldots, n.
$$
Here, $i$ indexes a sample. The data for sample $i$ are the output
$y_i \in \mathbf{R}$ and the $p$ inputs $x_{i1}, \ldots, x_{ip}$
stored as a vector $\mathbf{x}_i \in \mathbf{R}^p$. Typically, one
also includes an intercept term, and we will ignore this detail here
for simplicity (noting that it is easy to add and intercept without
fundamentally changing the model). The main quantities of interest are
the coefficients $b_1, \ldots, b_p$, which are stored as a vector,
$\mathbf{b} \in \mathbf{R}^p$. For example we might want to compute
the posterior mean of each $b_j$. This defines the likelihood.

Next we introduce the prior, which is that each of the coefficients is
normal with a mean of zero:
$$
b_j \sim N(0, \sigma_0^2).
$$
Here we have assumed for simplicity a single variance parameter shared
by all the coefficients.

### Posterior distribution

Skipping the derivations so that we can get more quickly to the topic
of interest, we note an important property of this model: the
posterior distribution of $\mathbf{b}$ is the multivariate normal
distribution with mean $\bar{\mathbf{b}}$ and covariance
$\mathbf{V}$, where
$$
\bar{\mathbf{b}} = \mathbf{V} \mathbf{X}^T\mathbf{y}/\sigma^2,
\qquad
\mathbf{V} = \sigma^2(\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I})^{-1},
$$
and where $\lambda = \frac{\sigma^2}{\sigma_0^2}$, $\mathbf{I}$ is the
$p \times p$ identity matrix, and $\mathbf{X}$ is the "input matrix",
that is, the $n \times p$ matrix formed by filling in each row $i$
with the vector $\mathbf{x}_i$.  Note that since everything is normal
in this model, the posterior mean is also the posterior mode.

Since the posterior distribution is multivariate normal with analytic
expressions for the posterior mean and posterior covariance, clearly
this is a case where Bayesian computational techniques such as MCMC or
variational inference are not needed. However, for very large data
sets (large $n$ and/or large $p$), computing the mean and covariance
can be very expensive—consider the effort involved in computing the
matrix product $\mathbf{X}^T \mathbf{X}$ and the matrix inverse in the
expression for $\mathbf{V}$—so actually these techniques could be
useful even if on paper the posterior distribution is straightforward.

### Simulating the posterior distribution

Potentially a igh-dimensional inference problem.

$$
b_j | b_{-j} \sim N(\mu_j, v_j)
$$
where
$$
\mu_j = ,
\qquad
v_j = x_j^Tx_j + \lambda
$$

TO DO: Connect to "variational view" of EM.

## Further reading

[Blei et al, "Variational inference: a review for statisticians"][blei-2017]

[blei-2017]: https://doi.org/10.1080/01621459.2017.1285773
[ryantibs-ridge]: https://github.com/ryantibs/statlearn-s24/
