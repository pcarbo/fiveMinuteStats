---
title: "Variational inference: approximations, objectives, and algorithms"
author: Peter Carbonetto
date: 2024-04-26
output: workflowr::wflow_html
---

## Introduction

Variational inference, like MCMC, is a technique for computing
posterior distributions and that are difficult to compute. However,
variational inference is typically aimed at *high-dimensional*
posterior inference problems where MCMC methods might struggle. The
tradeoff is that variational inference typically makes much stronger
approximations than MCMC methods and therefore one has to (or at least
should) think carefully about whether it is appropriate or justified
to use these techniques.

## Prerequisites

+ MCMC (specifically, Gibbs sampling) will be used to illustrate some
  of the key ideas without giving any background, so you should be
  familiar with MCMC ideas. MCMC is covered in several of the
  fiveMinuteStats vignettes.

+ Some of the derivations will use properties of the multivariate
  normal distribution, so you should be familiar with this as
  well. Again, this is covered in several fiveMinuteStats vignettes.

+ It is helpful, although not critical, if you are familiar with the
  basics of ridge regression. I don't think this was covered in any of
  the other fiveMinuteStats vignettes. See for example
  [Ryan Tibshirani's class notes][ryantibs-ridge] for an introduction.

## Ridge regression

We will use the ridge regression model as our running example to
illustrate the use of variational inference to perform posterior
inferences. Although variational inference methods aren't really
needed because the math works out very well for this model, its
convenient mathematical properties will be helpful for understanding
the variational approximations since we can compare the approximations
to the exact calculations. Ridge regression is also an example of a
high-dimensional inference problem where variational inference ideas
might be useful.

Although you may have seen ridge regression elsewhere, it hasn't been
introduced in any of the fiveMinuteStats vignettes, so we briefly
introduce it here.

There are different ways to introduce ridge regression. Here, we
introduce it as a Bayesian model; that is, we define a likelihood and
a prior, and we perform posterior inferences with respect to this
likelihood and prior.

The starting point is standard multiple linear regression model:
$$
y_i \sim N(\mathbf{x}_i^T\mathbf{b}, \sigma^2), \quad i = 1, \ldots, n.
$$
Here, $i$ indexes a sample, and the data for sample $i$ are the output
$y_i \in \mathbf{R}$ and the $p$ inputs $x_{i1}, \ldots, x_{ip}$
stored as a vector $\mathbf{x}_i \in \mathbf{R}^p$. Typically, one
also includes an intercept term, but we ignore this detail here
for simplicity (noting that it isn't hard to add and intercept without
fundamentally changing the model). The main quantities of interest are
the coefficients $b_1, \ldots, b_p$, which are stored as a vector,
$\mathbf{b} \in \mathbf{R}^p$. This defines the likelihood.

Next we introduce the prior, which is that each of the coefficients is
normal with a mean of zero:
$$
b_j \sim N(0, \sigma_0^2).
$$
Here we have assumed for simplicity a single variance parameter,
$\sigma^2$, that is shared by all the coefficients.

### Posterior distribution

Skipping the derivations so that we can get more quickly to the main
topic of interest, we note an important property of this model: the
posterior distribution of $\mathbf{b}$ is a multivariate normal with a mean $\bar{\mathbf{b}}$ and a covariance $\mathbf{V}$ as follows:
$$
\bar{\mathbf{b}} = \mathbf{V}
\mathbf{X}^T\mathbf{y}/\sigma^2
\quad \mathrm{and} \quad
\mathbf{V} = \sigma^2(\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I})^{-1},
$$
such that $\lambda = \sigma^2/\sigma_0^2$, $\mathbf{I}$ is the $p
\times p$ identity matrix, and $\mathbf{X}$ is the "input matrix",
that is, the $n \times p$ matrix formed by filling in each row $i$
with the vector $\mathbf{x}_i$. (Note that because the posterior is
multivariate normal, the posterior mean is also the posterior mode.)

Since the posterior distribution is multivariate normal with analytic
expressions for the posterior mean and posterior covariance, this is a
case where Bayesian computational techniques such as MCMC or
variational inference are not strictly needed. However, if we are
interested in analyzing a large data set—large $n$ and/or large
$p$—the computations could be a problem. For example, consider the
effort involved in computing the matrix product $\mathbf{X}^T
\mathbf{X}$ and the matrix inverse that appears in the expression for
$\mathbf{V}$. So actually these techniques could be useful even if on
paper the posterior distribution is straightforward.

## Simulating the ridge regression posterior distribution

As our first attempt at grappling with the challenges of inference in
high dimensions, let's consider a simple Gibbs sampler which involves
repeatedly choosing a dimension, $j$, and randomly sampling from the
posterior distribution of $b_j$ conditioned on all the other
dimensions:
$$
b_j \sim N(\mu_j, v_j^2),
$$
where
$$
v_j = \bigg(\frac{\mathbf{x}_j^T\mathbf{x}_j}{s^2} +
            \frac{1}{s_0^2}\bigg)^{-1}, \qquad
\mu_j = \frac{v_j}{s^2} \times
\bigg(\mathbf{x}_j^T\mathbf{y} -
\sum_{k \,\neq\, j} \mathbf{x}_j^T\mathbf{x}_k b_k \bigg).
$$

*Discuss the computational complexity of the Gibbs sampler updates for
ridge regression and compare to the computational complexity of the
analytical posterior computations above.*

Let's implement this Gibbs sampler and test it out on a moderately
large inference problem to gain some intuition for it, then we will
draw comparisons to the variational inference solution.

## The data set

Load the MASS package:

```{r mass}
library(MASS)
```

And set the seed to ensure the results are reproducible.

```{r set-seed} 
set.seed(3)
```

We will use this function to simulate some data from a ridge
regression model:

```{r sim-ridge-data}
# Simulate n data points from a ridge regression model with p inputs.
# Other parameters: p1, the number of nonzero coefficients to simulate
# (should not be greater than p); s, the residual standard deviation
# (s.d.); s0, the prior s.d. used to simulate the nonzero
# coefficients; and r, the correlation among the inputs.
sim_ridge_data <- function (n, p, p1, s, s0, r) {
  R <- matrix(r,p,p)
  diag(R) <- 1
  X <- mvrnorm(n,rep(0,p),R)
  X <- scale(X,center = TRUE,scale = FALSE)
  b <- rep(0,p)
  b[1:p1] <- rnorm(p1,sd = s0)
  y <- X %*% b + rnorm(n,sd = s)
  y <- drop(scale(y,center = TRUE,scale = FALSE))
  return(list(X = X,y = y,b = b))
}
```

Now simulate 80 data points from a ridge regression model with 24
inputs in which all but the first two inputs have coefficients of
zero. The 24 input variables are all quite strongly correlated with
each other (correlation of 0.8):

```{r sim-data}
n   <- 80
p   <- 24
s   <- 0.6
s0  <- 3
r   <- 0.8
sim <- sim_ridge_data(n,p,2,s,s0,r)
X   <- sim$X
y   <- sim$y
```

## A Gibbs sampler

This next bit of code defines a few functions used to implement the
Gibbs sampler, visualize the state of the Markov chain over time, and
compare to the analytical posterior distribution.

```{r ridge-gs}
# Perform "niter" Gibbs sampling updates for each input variable in
# the ridge regression model with data X, y. The Markov chain is
# initialized to "b".
ridge_gs <- function (X, y, s, s0, niter, b = rep(0,ncol(X))) {
  p  <- length(b)
  B  <- matrix(0,p,niter)
  XX <- crossprod(X)
  xy <- drop(crossprod(X,y))
  for (i in 1:niter) {
    for (j in 1:p) {
      v    <- 1/(XX[j,j]/s^2 + 1/s0^2)
      mu   <- v * (xy[j] - sum(XX[j,-j]*b[-j]))/s^2
      b[j] <- rnorm(1,mu,sqrt(v))
    }
    B[,i] <- b
  }
  return(B)
}

# Return the posterior distribution for the ridge regression model
# given data X, y.
ridge_post <- function (X, y, s, s0) {
  p <- ncol(X)
  lambda <- (s/s0)^2
  V <- s^2 * solve(crossprod(X) + lambda*diag(p))
  b <- drop(V %*% crossprod(X,y)/s^2)
  return(list(mean = b,var = diag(V)))
}

# This function is used to view how the parameter estimates change
# over time in the running of an inference algorithm (e.g.,
# MCMC). Input B is a p x niter matrix where p is the number of
# parameters and niter is the number of iterations performed.
# When show_average = TRUE, a running average is shown instead of
# the actual values in the B matrix.
plot_params_over_time <- function (B, show_average = FALSE) {
  p     <- nrow(B)
  niter <- ncol(B)
  if (show_average)
    B[1,] <- cumsum(B[1,])/1:niter
  plot(1:niter,B[1,],type = "l",lwd = 1,col = "dodgerblue",
       ylim = range(B),xlab = "iteration",ylab = "coefficient")
  for (j in 2:p) {
    if (show_average)
      B[j,] <- cumsum(B[j,])/1:niter
    lines(1:niter,B[j,],lwd = 1,col = "dodgerblue")
  }
}
```

Let's run the Gibbs sampler for 100 iterations:

```{r run-ridge-gs}
niter <- 100
B <- ridge_gs(X,y,s,s0,niter)
```

Now plot the state of the Markov chain over time, and compare the
final state of the Markov chain to the exact posterior mean which we
are able to compute because the posterior is multivariate normal (the
exact means are the black dots in the plot):

```{r plot-ridge-gs, fig.height=5, fig.width=3}
par(mar = c(4,4,1,0))
plot_params_over_time(B)
post <- ridge_post(X,y,s,s0)
points(rep(niter,p),post$mean,pch = 20,col = "darkblue",cex = 1)
```

At the last iteration, the Markov chain is quite close to the exact
posterior mean. But we wouldn't expect it to be exactly the same
because the MCMC is intended to simulate the full posterior
distribution, not just recover the posterior mean. However, if we
instead take an average of the Markov chain states, then we should (or
hopefully) get closer to the exact calculations. This next plot shows
the running average across the 100 Gibbs sampler iterations:

```{r plot-ridge-gs-running-average, fig.height=5, fig.width=3}
par(mar = c(4,4,1,0))
plot_params_over_time(B,show_average = TRUE)
points(rep(niter,p),post$mean,pch = 20,col = "darkblue",cex = 1)
```

## A different iterative algorithm

Let's now consider a different iterative algorithm: on the one hand,
like the Gibbs sampler, this new algorithm updates one co-ordinate or
dimension at a time; on the other hand, unlike the Gibbs sampler, the
updates are *deterministic*. That is, given the same inputs, this
iterative algorithm will *always produce the same output.* To start,
we will simply describe this iterative algorithm, and later we will
motivate it as fitting a variational approxiation to the (exact)
posterior distribution.

### Posterior distribution for the "single-input" ridge regression model

To describe the algorithm, it will be helpful to first write down the
posterior distribution for a ridge regression model with a single
input (that is, $p = 1$), which is a special case of the expressions
given above:
$$
\bar{b} = \frac{v \mathbf{x}^T\mathbf{y}}{\sigma^2},
\qquad
v = \frac{\sigma^2}{\mathbf{x}^T\mathbf{x} + \lambda}.
$$
For describing the updates, it will be convenient to define these
expressions as functions of the data, so let's change the notation
slightly:
$$
\bar{b}(\mathbf{x},\mathbf{y}) =
\frac{v(\mathbf{x}, \mathbf{y}) \, \mathbf{x}^T\mathbf{y}}{\sigma^2},
\qquad
v(\mathbf{x},\mathbf{y}) = \frac{\sigma^2}
{\mathbf{x}^T\mathbf{x} + \lambda}.
$$

### The coordinatewise updates

With these expressions, the coordinatewise updates for the iterative
algorithm involve two steps:
$$
\mathbf{r}_j \leftarrow \mathbf{y} - \sum_{k\, \neq\, j} \mathbf{x}_k b_k,
\qquad
b_j \leftarrow \bar{b}(\mathbf{x}_j, \mathbf{r}_j)
$$

Let's now see what these updates look like on the example data set.

The first function here computes the posterior mean and variance for
the single-input ridge regression model and the second function is
simply a pair of for-loops that repeatedly cycles through the
single-coordinate updates for all the $p$ input variables.

```{r ridge-iterative}
# Return the posterior distribution for the single-input
# ridge regression model given data x, y.
ridge1_post <- function (x, y, s, s0) {
  xx <- sum(x^2)
  xy <- sum(x*y)
  v <- s^2/(xx + (s/s0)^2)
  b <- v*xy/s^2
  return(list(mean = b,var = v))
}

# Perform "niter" updates of the iterative algorithm for ridge
# regression, initialized to "b".
ridge_iterative <- function (X, y, s, s0, niter, b = rep(0,ncol(X))) {
  p <- length(b)
  B <- matrix(0,p,niter)
  for (i in 1:niter) {
    r <- drop(y - X %*% b)
    for (j in 1:p) {
      x <- X[,j]
      r <- r + x*b[j]
      b[j] <- ridge1_post(x,r,s,s0)$mean
      r <- r - x*b[j]
    }
    B[,i] <- b
  }
  return(B)
}
```

Let's now run 20 rounds of this updates and look at how the
coefficients $b_j$ change over time:

```{r plot-ridge-iterative, fig.height=5, fig.width=3}
par(mar = c(4,4,1,0))
niter <- 20
B <- ridge_iterative(X,y,s,s0,niter)
plot_params_over_time(B)
points(rep(niter,p),post$mean,pch = 20,col = "darkblue",cex = 1)
```

Notice that the iterative estimates of the coefficients get
progressively closer to the analytical posterior mean. What happens if
you run the iterative algorithm longer? Does it ever recover the
analytical solution? Do the iterates eventually stop changing (that
is, do they "converge" to a fixed point)? Here, we started at an
initial estimate where all the coefficients were zero. What happens if
we give the algorithm a random starting point?

TO DO: Discuss differences between this as the Gibbs sampler (e.g., no
need to take running averages). Do these updates always converge to a
fixed point?

TO DO: Discuss computational complexity of this algorithm compard
to the Gibbs sampler and the analytical posterior.

TO DO: First think of this as computing the MAP (maximum a
posteriori).  Next think of this as computing approximate posterior
means. How to think about this?

## Further reading

+ Blei et al, [Variational inference: a review for statisticians.][blei-2017]

+ For a statistical physics perspective, see Yedidia et al,
[Constructing free-energy approximations and generalized belief
propagation algorithms.][yedidia-2005]

[blei-2017]: https://doi.org/10.1080/01621459.2017.1285773
[ryantibs-ridge]: https://github.com/ryantibs/statlearn-s24/
[yedidia-2005]: https://doi.org/10.1109/TIT.2005.850085
