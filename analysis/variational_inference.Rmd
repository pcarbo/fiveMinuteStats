---
title: "Variational inference: approximations, objectives, and algorithms"
author: Peter Carbonetto
date: 2024-04-26
output: workflowr::wflow_html
---

## Prerequisites

+ MCMC sampling (and specifically Gibbs sampling) will be used to
  illustrate some of the key ideas so you should be familiar with
  those.

+ Properties of multivariate Gaussians.

+ It is helpful if you are familiar with ridge regression, but this
  isn't essential since we will (re)introduce it here. See for example
  [Ryan Tibshirani's class notes][ryantibs-ridge].

## Introduction

*Add introduction here. Talk about this idea of solving a
 high-dimensional/large-scale inference problem."

## Ridge regression

We will use the ridge regression model as our running example to
illustrate the use of variational inference to perform posterior
inferences. Although variational inference methods aren't really
needed because the math works out very well for this model, its
convenient mathematical properties will be helpful for understanding
the variational approximations since we can compare the approximations
to the exact calculations. Ridge regression is also an example of a
high-dimensional inference problem where variational inference ideas
might be useful.

Although you may have seen ridge regression elsewhere, it hasn't been
introduced in any of the fiveMinuteStats vignettes, so we briefly
introduce it here.

There are different ways to introduce ridge regression. Here, we
introduce it as a Bayesian model; that is, we define a likelihood and
a prior, and we perform posterior inferences with respect to this
likelihood and prior.

The starting point is standard multiple linear regression model:
$$
y_i \sim N(\mathbf{x}_i^T\mathbf{b}, \sigma^2), \quad i = 1, \ldots, n.
$$
Here, $i$ indexes a sample, and the data for sample $i$ are the output
$y_i \in \mathbf{R}$ and the $p$ inputs $x_{i1}, \ldots, x_{ip}$
stored as a vector $\mathbf{x}_i \in \mathbf{R}^p$. Typically, one
also includes an intercept term, but we ignore this detail here
for simplicity (noting that it isn't hard to add and intercept without
fundamentally changing the model). The main quantities of interest are
the coefficients $b_1, \ldots, b_p$, which are stored as a vector,
$\mathbf{b} \in \mathbf{R}^p$. This defines the likelihood.

Next we introduce the prior, which is that each of the coefficients is
normal with a mean of zero:
$$
b_j \sim N(0, \sigma_0^2).
$$
Here we have assumed for simplicity a single variance parameter,
$\sigma^2$, that is shared by all the coefficients.

### Posterior distribution

Skipping the derivations so that we can get more quickly to the main
topic of interest, we note an important property of this model: the
posterior distribution of $\mathbf{b}$ is a multivariate normal with a mean $\bar{\mathbf{b}}$ and a covariance $\mathbf{V}$ as follows:
$$
\bar{\mathbf{b}} = \mathbf{V}
\mathbf{X}^T\mathbf{y}/\sigma^2
\quad \mathrm{and} \quad
\mathbf{V} = \sigma^2(\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I})^{-1},
$$
such that $\lambda = \sigma^2/\sigma_0^2$, $\mathbf{I}$ is the $p
\times p$ identity matrix, and $\mathbf{X}$ is the "input matrix",
that is, the $n \times p$ matrix formed by filling in each row $i$
with the vector $\mathbf{x}_i$. (Note that because the posterior is
multivariate normal, the posterior mean is also the posterior mode.)

Since the posterior distribution is multivariate normal with analytic
expressions for the posterior mean and posterior covariance, this is a
case where Bayesian computational techniques such as MCMC or
variational inference are not strictly needed. However, if we are
interested in analyzing a large data set—large $n$ and/or large
$p$—the computations could be a problem. For example, consider the
effort involved in computing the matrix product $\mathbf{X}^T
\mathbf{X}$ and the matrix inverse that appears in the expression for
$\mathbf{V}$. So actually these techniques could be useful even if on
paper the posterior distribution is straightforward.

## Simulating the posterior distribution

Potentially a high-dimensional inference problem.

$$
b_j | b_{-j} \sim N(\mu_j, v_j)
$$
where
$$
\mu_j = ,
\qquad
v_j = x_j^Tx_j + \lambda
$$

TO DO: Connect to "variational view" of EM.

## Further reading

[Blei et al, "Variational inference: a review for statisticians"][blei-2017]

For a statistical physics perspective, see [Yedidia et al,
"Constructing free-energy approximations and generalized belief
propagation algorithms"][yedidia-2005].

[blei-2017]: https://doi.org/10.1080/01621459.2017.1285773
[ryantibs-ridge]: https://github.com/ryantibs/statlearn-s24/
[yedidia-2005]: https://doi.org/10.1109/TIT.2005.850085
