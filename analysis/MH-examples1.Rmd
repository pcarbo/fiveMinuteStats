---
title: Simple examples of the Metropolis-Hastings algorithm
author: 
  name: Matthew Stephens
  affiliation: University of Chicago
date: January 23, 2026
output:
  pdf_document:
    keep_tex: false
    latex_engine: pdflatex
    template: readable.tex
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
spacing: single
graphics: yes
endnote: no
---

See [here][pdf_version] for a PDF version of this vignette.

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,fig.align = "center")
```

Prerequisites
=============

You should be familiar with the Metropolis-Hastings algorithm,
introduced [here][MH_intro], and elaborated on [here][MH_intro_02].

Caveat on code
--------------

Note that the code is designed to be readable by a beginner, rather
than "efficient". The idea is that you can use this code to learn
about the basics of MCMC, but not as a model for how to program well
in R!

Example 1: Sampling from an exponential distribution using MCMC
===============================================================

Any MCMC scheme aims to produce (dependent) samples from a "target"
distribution. In this case we are going to use the exponential
distribution with mean 1 as our target distribution. Here we define
this function (on the log scale):

```{r log-exp-target}
log_exp_target <- function (x)
  dexp(x,rate = 1,log = TRUE)
```

The following code implements a simple M-H algorithm. (Note that the
parameter "log_target" is a function which computes the log of the
target distribution; you may be unfamiliar with the idea of passing a
function as a parameter, but it works just like any other type of
parameter)

```{r easy-mcmc}
easyMCMC <- function (log_target, niter, startval, proposalsd) {
  x <- rep(0,niter)
  x[1] <- startval
  for (i in 2:niter) {
    currentx <- x[i-1]
    proposedx <- rnorm(1,currentx,proposalsd) 
    A <- exp(log_target(proposedx) - log_target(currentx))
	u <- runif(1)
    if(u < A)
      x[i] <- proposedx
    else
      x[i] <- currentx
  }
  return(x)
}
```

Now we run the MCMC three times from different starting points and
compare the results:

```{r run-easy-mcmc, fig.height=3, fig.width=6}
par(mfrow = c(1,2))
z1 <- easyMCMC(log_exp_target,1000,3,1)
z2 <- easyMCMC(log_exp_target,1000,1,1)
z3 <- easyMCMC(log_exp_target,1000,5,1)
plot(z1,type = "l",xlab = "time",ylab = "x")
lines(z2,col = 2)
lines(z3,col = 3)
plot(log_exp_target(z1),xlab = "time",ylab = "log target")
lines(log_exp_target(z2),col = 2)
lines(log_exp_target(z3),col = 3)
```

```{r summarize-easy-mcmc, fig.height=2.5, fig.width=7}
par(mfrow = c(1,3))
maxz <- max(c(z1,z2,z3))
hist(z1,breaks = seq(0,maxz,length.out = 20),xlab = "x")
hist(z2,breaks = seq(0,maxz,length.out = 20),xlab = "x")
hist(z3,breaks = seq(0,maxz,length.out = 20),xlab = "x")
```

Exercise
--------

Use the function "easyMCMC" to explore the following:

a. How do different starting values affect the MCMC scheme? (Try some
extreme starting points.)

b. What is the effect of having a bigger (or smaller) proposal
standard deviation? (Again, try some extreme values.)

c. Try changing the (log) target function to "log_target_bimodal"
below. What does this target distribution look like?  What happens if
the proposal standard deviation is too small here? (e.g., try 1
and 0.1 and see what happens)

```{r log-target-bimodal}
log_target_bimodal <- function (x)
  log(0.8*dnorm(x,-4,1) + 0.2*dnorm(x,4,1))
```

Example 2: Estimating an allele frequency
=========================================

A standard assumption when modelling genotypes of biallelic loci
(e.g., loci with alleles $A$ and $a$) is that the population is
"mating at random". From this assumption, it follows that the population
will be in "Hardy-Weinberg equilibrium" (HWE), which means that if $p$
is the frequency of the $A$ allele, then the genotypes $AA$, $Aa$ and
$aa$ have frequencies $p^2$, $2p(1-p)$ and $(1-p)^2$, respectively.

A simple prior for $p$ is to assume it is uniform on $[0,1]$. Suppose
that we sample $n$ individuals, and observe $n_{AA}$ individuals with
genotype $AA$, $n_{Aa}$ individuals with genotype $Aa$, and $n_{aa}$
individuals with genotype $aa$.

The following R code implements a M-H algorithm to sample from the
posterior distribution of $p$. Try to go through the code to see how
it works.

The first function returns the (log) prior density:

```{r log-prior}
log_prior <- function (p) {
  if((p < 0) || (p > 1))
    return(-Inf)
  else
    return(0)
}
```

The second function returns the (log) likelihood:

```{r log-likelihood-hwe}
log_likelihood_hwe <- function (p, nAA, nAa, naa)
  return(2*nAA*log(p) +
         nAa*log(2*p*(1-p)) +
		 2*naa*log(1-p))
```

And now this function implements the M-H algorithm:

```{r psampler}
psampler <- function (nAA, nAa, naa, niter, pstartval, pproposalsd) {
  p <- rep(0,niter)
  p[1] <- pstartval
  for (i in 2:niter) {
    currentp <- p[i-1]
    newp <- currentp + rnorm(1,0,pproposalsd)
    A <- exp(log_prior(newp)
	         - log_prior(currentp)
	         + log_likelihood_hwe(newp,nAA,nAa,naa)
			 - log_likelihood_hwe(currentp,nAA,nAa,naa))
    if (runif(1) < A)
      p[i] <- newp
    else
      p[i] <- currentp
  }
  return(p)
}
```

Try running this algorithm for $n_{AA} = 50$, $n_{Aa} = 21$, $n_{aa}
= 29$:

```{r run-psampler}
z <- psampler(50,21,29,10000,0.5,0.01)
```

Now here's some R code to compare the samples generated by the M-H
algorithm with the theoretical posterior. In this case, the
theoretical posterior is available analytically; since we observed $A$
121 times and $a$ 79 times, the posterior for $p$ is
$\mathrm{Beta}(121 + 1,79 + 1)$.

```{r psampler-hist, fig.height=3, fig.width=3}
x <- seq(0,1,length.out = 1000)
hist(z,breaks = 32,probability = TRUE,xlab = "x",main = "")
lines(x,dbeta(x,122,80))
```

You might also like to discard the first 5,000 samples as "burnin":

```{r, fig.height=3, fig.width=3}
hist(z[5001:10000],breaks = 16,probability = TRUE,xlab = "x",main = "")
lines(x,dbeta(x,122,80))
```

## Exercise

Investigate how the starting point and proposal standard deviation affect the convergence of the algorithm. 

Example 3: Estimating an allele frequency and inbreeding coefficient
====================================================================

A slightly more complex alternative than HWE is to assume that
there is a tendency for people to mate with others who are 
slightly more closely-related than "random" (as might
happen in a geographically-structured population, for example).
This will result in an excess of homozygotes compared with
HWE. A simple way to capture this is to introduce an extra parameter,
the "inbreeding coefficient" $f$, and assume that the genotypes
$AA$, $Aa$ and $aa$ have frequencies $fp + (1-f)p*p, (1-f) 2p(1-p)$,
and $f(1-p) + (1-f)(1-p)(1-p)$.

In most cases it would be natural to treat $f$ as a feature of
the population, and therefore assume $f$ is constant across loci.
For simplicity we will consider just a single locus.

Note that both $f$ and $p$ are constrained to lie between 0 and 1
(inclusive). A simple prior for each of these two parameters is to assume
that they are independent, uniform on $[0,1]$. Suppose
that we sample $n$ individuals, and observe $n_{AA}$ with genotype $AA$,
$n_{Aa}$ with genotype $Aa$ and $n_{aa}$ with genotype $aa$. 

Exercise
--------

- Write a short MCMC routine to sample from the joint
distribution of $f$ and $p$. 

Hint: here is a start; you'll need to fill in the ...

```{r eval=FALSE}
# The first step is probably to code a log-likelihood function for the inbreeding model....
log_likelihood_inbreeding = function(...){
  ...
}

# then use the log-likelihood within your MCMC scheme
fpsampler = function(nAA, nAa, naa, niter, fstartval, pstartval, fproposalsd, pproposalsd){
  f = rep(0,niter)
  p = rep(0,niter)
  f[1] = fstartval
  p[1] = pstartval
  for(i in 2:niter){
    currentf = f[i-1]
    currentp = p[i-1]
    newf = currentf + ...
    newp = currentp + ...
    ...
  }
  return(list(f=f,p=p)) # return a "list" with two elements named f and p
}
```

- Use this sample to obtain point estimates for
$f$ and $p$ (e.g. using posterior means) and interval estimates for both $f$ and $p$
(e.g. 90% posterior credible intervals), when the data are $n_{AA} = 50, n_{Aa} = 21, n_{aa}=29$.

Addendum: Gibbs sampling
========================

You could also tackle this problem with a Gibbs Sampler (see vignettes
[here](gibbs1.Rmd) and [here](gibbs2.Rmd)).

To do so you will want to use the following "latent variable"
representation of the model:

$$
z_i \sim Bernoulli(f)
$$

$$
p(g_i=AA | z_i=1) = p; p(g_i=AA | z_i=0) = p^2
$$

$$
p(g_i=Aa | z_i = 1)= 0; p(g_i=Aa | z_i=0) = 2p(1-p)
$$

$$
p(g_i=aa | z_i = 1) = (1-p); p(g_i =aa | z_i=0) = (1-p)^2
$$

Summing over $z_i$ gives the same model as above:
$$
p(g_i=AA) = fp + (1-f)p^2
$$

Exercise
--------

Using the above, implement a Gibbs Sampler to sample from the joint
distribution of $z,f,$ and $p$ given genotype data $g$.

Hint: this requires iterating the following steps
  
1. sample $z$ from $p(z | g, f, p)$
  
2. sample $f,p$ from $p(f, p | g, z)$

[pdf_version]: https://github.com/pcarbo/fiveMinuteStats/blob/master/docs/MH-examples1.pdf
[MH_intro]: https://pcarbo.github.io/fiveMinuteStats/MH_intro.html
[MH_intro_02]: https://pcarbo.github.io/fiveMinuteStats/MH_intro_02.html
