<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Peter Carbonetto" />

<meta name="date" content="2024-04-26" />

<title>Variational inference: approximations, objectives, and algorithms</title>

<script src="site_libs/header-attrs-2.26/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-6.4.2/css/all.min.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet" />

<link rel="icon" href="https://github.com/workflowr/workflowr-assets/raw/main/img/reproducible.png">
<!-- Add a small amount of space between sections. -->
<style type="text/css">
div.section {
  padding-top: 12px;
}
</style>



<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">fiveMinuteStats</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="license.html">License</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/stephens999/fiveMinuteStats">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Variational inference: approximations,
objectives, and algorithms</h1>
<h4 class="author">Peter Carbonetto</h4>
<h4 class="date">2024-04-26</h4>

</div>


<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-report" data-toggle="collapse" data-target="#workflowr-report">
<span class="glyphicon glyphicon-list" aria-hidden="true"></span>
workflowr <span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span>
</button>
</p>
<div id="workflowr-report" class="collapse">
<ul class="nav nav-tabs">
<li class="active">
<a data-toggle="tab" href="#summary">Summary</a>
</li>
<li>
<a data-toggle="tab" href="#checks"> Checks <span
class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span>
</a>
</li>
<li>
<a data-toggle="tab" href="#versions">Past versions</a>
</li>
</ul>
<div class="tab-content">
<div id="summary" class="tab-pane fade in active">
<p>
<strong>Last updated:</strong> 2025-03-06
</p>
<p>
<strong>Checks:</strong> <span
class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> 7
<span class="glyphicon glyphicon-exclamation-sign text-danger"
aria-hidden="true"></span> 0
</p>
<p>
<strong>Knit directory:</strong> <code>fiveMinuteStats/analysis/</code>
<span class="glyphicon glyphicon-question-sign" aria-hidden="true"
title="This is the local directory in which the code in this file was executed.">
</span>
</p>
<p>
This reproducible <a href="https://rmarkdown.rstudio.com">R Markdown</a>
analysis was created with <a
  href="https://github.com/workflowr/workflowr">workflowr</a> (version
1.7.1). The <em>Checks</em> tab describes the reproducibility checks
that were applied when the results were created. The <em>Past
versions</em> tab lists the development history.
</p>
<hr>
</div>
<div id="checks" class="tab-pane fade">
<div id="workflowr-checks" class="panel-group">
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRMarkdownfilestronguptodate">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>R Markdown file:</strong> up-to-date
</a>
</p>
</div>
<div id="strongRMarkdownfilestronguptodate"
class="panel-collapse collapse">
<div class="panel-body">
<p>Great! Since the R Markdown file has been committed to the Git
repository, you know the exact version of the code that produced these
results.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongEnvironmentstrongempty">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Environment:</strong> empty </a>
</p>
</div>
<div id="strongEnvironmentstrongempty" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! The global environment was empty. Objects defined in the
global environment can affect the analysis in your R Markdown file in
unknown ways. For reproduciblity it’s best to always run the code in an
empty environment.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSeedstrongcodesetseed12345code">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Seed:</strong>
<code>set.seed(12345)</code> </a>
</p>
</div>
<div id="strongSeedstrongcodesetseed12345code"
class="panel-collapse collapse">
<div class="panel-body">
<p>The command <code>set.seed(12345)</code> was run prior to running the
code in the R Markdown file. Setting a seed ensures that any results
that rely on randomness, e.g. subsampling or permutations, are
reproducible.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSessioninformationstrongrecorded">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Session information:</strong>
recorded </a>
</p>
</div>
<div id="strongSessioninformationstrongrecorded"
class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Recording the operating system, R version, and package
versions is critical for reproducibility.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongCachestrongnone">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Cache:</strong> none </a>
</p>
</div>
<div id="strongCachestrongnone" class="panel-collapse collapse">
<div class="panel-body">
<p>Nice! There were no cached chunks for this analysis, so you can be
confident that you successfully produced the results during this
run.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongFilepathsstrongrelative">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>File paths:</strong> relative </a>
</p>
</div>
<div id="strongFilepathsstrongrelative" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Using relative paths to the files within your workflowr
project makes it easier to run your code on other machines.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRepositoryversionstrongahrefhttpsgithubcompcarbofiveMinuteStatstreeb283fe5e325148ff2ada2dfc25cf2b72bccb25f3targetblankb283fe5a">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Repository version:</strong>
<a href="https://github.com/pcarbo/fiveMinuteStats/tree/b283fe5e325148ff2ada2dfc25cf2b72bccb25f3" target="_blank">b283fe5</a>
</a>
</p>
</div>
<div
id="strongRepositoryversionstrongahrefhttpsgithubcompcarbofiveMinuteStatstreeb283fe5e325148ff2ada2dfc25cf2b72bccb25f3targetblankb283fe5a"
class="panel-collapse collapse">
<div class="panel-body">
<p>
Great! You are using Git for version control. Tracking code development
and connecting the code version to the results is critical for
reproducibility.
</p>
<p>
The results in this page were generated with repository version
<a href="https://github.com/pcarbo/fiveMinuteStats/tree/b283fe5e325148ff2ada2dfc25cf2b72bccb25f3" target="_blank">b283fe5</a>.
See the <em>Past versions</em> tab to see a history of the changes made
to the R Markdown and HTML files.
</p>
<p>
Note that you need to be careful to ensure that all relevant files for
the analysis have been committed to Git prior to generating the results
(you can use <code>wflow_publish</code> or
<code>wflow_git_commit</code>). workflowr only checks the R Markdown
file, but you know if there are other scripts or data files that it
depends on. Below is the status of the Git repository when the results
were generated:
</p>
<pre><code>
Ignored files:
    Ignored:    analysis/variational_inference.pdf

</code></pre>
<p>
Note that any generated files, e.g. HTML, png, CSS, etc., are not
included in this status report because it is ok for generated content to
have uncommitted changes.
</p>
</div>
</div>
</div>
</div>
<hr>
</div>
<div id="versions" class="tab-pane fade">

<p>
These are the previous versions of the repository in which changes were
made to the R Markdown (<code>analysis/variational_inference.Rmd</code>)
and HTML (<code>docs/variational_inference.html</code>) files. If you’ve
configured a remote Git repository (see <code>?wflow_git_remote</code>),
click on the hyperlinks in the table below to view the files as they
were in that past version.
</p>
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
File
</th>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
<th>
Message
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/pcarbo/fiveMinuteStats/blob/b283fe5e325148ff2ada2dfc25cf2b72bccb25f3/analysis/variational_inference.Rmd" target="_blank">b283fe5</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-06
</td>
<td>
Added some closing notes to the variational inference vignette.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/pcarbo/fiveMinuteStats/blob/7e9a07ed99dc8a7e81c815b8f91ce92c288d4a9b/analysis/variational_inference.Rmd" target="_blank">7e9a07e</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-06
</td>
<td>
Added proof sketch.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/pcarbo/fiveMinuteStats/blob/6c98bf6a0f6563172171f9daf2ffb3fe07b370d9/analysis/variational_inference.Rmd" target="_blank">6c98bf6</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-06
</td>
<td>
Added details on the fully-factorized variational approximation.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/pcarbo/fiveMinuteStats/blob/936aed8e3059a9980ba24a1835d4841f21e30cfa/analysis/variational_inference.Rmd" target="_blank">936aed8</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-06
</td>
<td>
Fixed alignment of the multi-line eqs.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/pcarbo/fiveMinuteStats/blob/6fca4dc752cfd04dd36c21213001eaa8ac664498/analysis/variational_inference.Rmd" target="_blank">6fca4dc</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-06
</td>
<td>
Started derivation of the ELBO.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/pcarbo/fiveMinuteStats/blob/4b10e2bce6323d8ce25798f540442ff915446f7e/analysis/variational_inference.Rmd" target="_blank">4b10e2b</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-06
</td>
<td>
Added an example of running the CAVI algorithm.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/pcarbo/fiveMinuteStats/blob/d8f1cda9f0ac128bb3c918d85120837e3989547c/analysis/variational_inference.Rmd" target="_blank">d8f1cda</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-06
</td>
<td>
A few edits to the variational inference vignette.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/pcarbo/fiveMinuteStats/blob/548b6766cf36e0e7160cd2a58d5242339f35c917/analysis/variational_inference.Rmd" target="_blank">548b676</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-06
</td>
<td>
Wrote intro for variational inference vignette.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/pcarbo/fiveMinuteStats/blob/9158e7e34ca9f5c2e345662b38d9b54688162745/analysis/variational_inference.Rmd" target="_blank">9158e7e</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-06
</td>
<td>
Added iterative algorithm to the variational inference vignette.
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/pcarbo/fiveMinuteStats/9158e7e34ca9f5c2e345662b38d9b54688162745/docs/variational_inference.html" target="_blank">9158e7e</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-06
</td>
<td>
Added iterative algorithm to the variational inference vignette.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/pcarbo/fiveMinuteStats/blob/651132834318fced49146b70a949ab47edd6e3fb/analysis/variational_inference.Rmd" target="_blank">6511328</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-06
</td>
<td>
Added CAVI updates.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/pcarbo/fiveMinuteStats/blob/4f311ff0e5db078fbfb0ac63cf3b38ae4faf5cff/analysis/variational_inference.Rmd" target="_blank">4f311ff</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-05
</td>
<td>
Revised some of the initial parts of the variational inference vignette.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/pcarbo/fiveMinuteStats/blob/2ef9b10c84372d4c095b8fcb86cc51e3683fa8e6/analysis/variational_inference.Rmd" target="_blank">2ef9b10</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-05
</td>
<td>
Added link to the variational inference vignette.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/pcarbo/fiveMinuteStats/blob/58b62f0528e9adff557385a2cadc03b707bfc21a/analysis/variational_inference.Rmd" target="_blank">58b62f0</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-05
</td>
<td>
A few edits to the variational inference code.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/pcarbo/fiveMinuteStats/blob/d1311c64c43b008e86cecb4bfd1ab110f588419d/analysis/variational_inference.Rmd" target="_blank">d1311c6</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-05
</td>
<td>
Implemented ridge_coord_ascent() function for the variational inference
vignette.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/pcarbo/fiveMinuteStats/blob/283a037ae2f69b78f42ad1c16d0d6791b3d189a2/analysis/variational_inference.Rmd" target="_blank">283a037</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-05
</td>
<td>
Added code to perform Gibbs sampling in the variational inference
vignette.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/pcarbo/fiveMinuteStats/blob/c97d875118bd2331597c5672fa3a51d27ee2aa79/analysis/variational_inference.Rmd" target="_blank">c97d875</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-05
</td>
<td>
Implemented function ridge_post() for the variational inference
vignette.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/pcarbo/fiveMinuteStats/blob/0e04740589371d9a1913f45e5cc033c85940bdb1/analysis/variational_inference.Rmd" target="_blank">0e04740</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-05
</td>
<td>
Defined X.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/pcarbo/fiveMinuteStats/blob/b46d1604abffe8260a502fb44a7bc951d2df2630/analysis/variational_inference.Rmd" target="_blank">b46d160</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-05
</td>
<td>
A few edits to the ridge regression model.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/pcarbo/fiveMinuteStats/blob/7174694260f63e6210de3f939cab38af0719e777/analysis/variational_inference.Rmd" target="_blank">7174694</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-05
</td>
<td>
Added a few more rough details about the ridge regression model.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/pcarbo/fiveMinuteStats/blob/4896369523531ca3a658ae091a61fb5675cb2fc0/analysis/variational_inference.Rmd" target="_blank">4896369</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-05
</td>
<td>
Added some details about the ridge regression model to the
variational_inference vignette.
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/pcarbo/fiveMinuteStats/19190b0b15e98097a0af23325cfb1d8350b42de2/docs/variational_inference.html" target="_blank">19190b0</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-05
</td>
<td>
First build of the variational inference vignette.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/pcarbo/fiveMinuteStats/blob/1352fc2405aa947a6428a657e49b7c1b1ca65b8c/analysis/variational_inference.Rmd" target="_blank">1352fc2</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-05
</td>
<td>
workflowr::wflow_publish("variational_inference.Rmd", verbose = TRUE)
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/pcarbo/fiveMinuteStats/blob/e1a2bea303414f5a508b5041876bee72aec3f28b/analysis/variational_inference.Rmd" target="_blank">e1a2bea</a>
</td>
<td>
Peter Carbonetto
</td>
<td>
2025-03-05
</td>
<td>
workflowr::wflow_publish("index.Rmd")
</td>
</tr>
</tbody>
</table>
</div>
<hr>
</div>
</div>
</div>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Variational inference, like MCMC, is a technique for computing
posterior distributions and that are difficult to compute. However,
variational inference is typically aimed at <em>high-dimensional</em>
posterior inference problems where MCMC methods might struggle. The
tradeoff is that variational inference typically makes much stronger
approximations than MCMC methods and therefore one has to (or at least
should) think carefully about whether it is appropriate or justified to
use these techniques.</p>
<p>I’d like to acknowledge that some of the content of this vignette is
based on notes from John Novembre and Matthew Stephens.</p>
</div>
<div id="prerequisites" class="section level2">
<h2>Prerequisites</h2>
<ul>
<li><p>You should be familiar with the basic concepts of Bayesian
inference such as Bayes’ Theorem and the posterior
distribution.</p></li>
<li><p>MCMC (specifically, Gibbs sampling) will be used to illustrate
some of the key ideas without giving any background, so you should be
familiar with MCMC ideas. MCMC is covered in several of the
fiveMinuteStats vignettes.</p></li>
<li><p>Some of the derivations will use properties of the multivariate
normal distribution, so you should be familiar with this as well. Again,
this is covered in several fiveMinuteStats vignettes.</p></li>
<li><p>It is helpful, although not critical, if you are familiar with
the basics of ridge regression. I don’t think this was covered in any of
the other fiveMinuteStats vignettes. See for example <a
href="https://github.com/ryantibs/statlearn-s24/">Ryan Tibshirani’s
class notes</a> for an introduction.</p></li>
</ul>
</div>
<div id="ridge-regression" class="section level2">
<h2>Ridge regression</h2>
<p>We will use the ridge regression model as our running example to
illustrate the use of variational inference to perform posterior
inferences. Although variational inference methods aren’t really needed
because the math works out very well for this model, its convenient
mathematical properties will be helpful for understanding the
variational approximations since we can compare the approximations to
the exact calculations. Ridge regression is also an example of a
high-dimensional inference problem where variational inference ideas
might be useful.</p>
<p>Although you may have seen ridge regression elsewhere, it hasn’t been
introduced in any of the fiveMinuteStats vignettes, so we briefly
introduce it here.</p>
<p>There are different ways to introduce ridge regression. Here, we
introduce it as a Bayesian model; that is, we define a likelihood and a
prior, and we perform posterior inferences with respect to this
likelihood and prior.</p>
<p>The starting point is standard multiple linear regression model:
<span class="math display">\[
y_i \sim N(\mathbf{x}_i^T\mathbf{b}, \sigma^2), \quad i = 1, \ldots, n.
\]</span> Here, <span class="math inline">\(i\)</span> indexes a sample,
and the data for sample <span class="math inline">\(i\)</span> are the
output <span class="math inline">\(y_i \in \mathbf{R}\)</span> and the
<span class="math inline">\(p\)</span> inputs <span
class="math inline">\(x_{i1}, \ldots, x_{ip}\)</span> stored as a vector
<span class="math inline">\(\mathbf{x}_i \in \mathbf{R}^p\)</span>.
Typically, one also includes an intercept term, but we ignore this
detail here for simplicity (noting that it isn’t hard to add and
intercept without fundamentally changing the model). The main quantities
of interest are the coefficients <span class="math inline">\(b_1,
\ldots, b_p\)</span>, which are stored as a vector, <span
class="math inline">\(\mathbf{b} \in \mathbf{R}^p\)</span>. This defines
the likelihood.</p>
<p>Next we introduce the prior, which is that each of the coefficients
is normal with a mean of zero: <span class="math display">\[
b_j \sim N(0, \sigma_0^2).
\]</span> Here we have assumed for simplicity a single variance
parameter, <span class="math inline">\(\sigma^2\)</span>, that is shared
by all the coefficients.</p>
<div id="posterior-distribution" class="section level3">
<h3>Posterior distribution</h3>
<p>Skipping the derivations so that we can get more quickly to the main
topic of interest, we note an important property of this model: the
posterior distribution of <span
class="math inline">\(\mathbf{b}\)</span> is a multivariate normal with
a mean <span class="math inline">\(\bar{\mathbf{b}}\)</span> and a
covariance <span class="math inline">\(\mathbf{V}\)</span> as follows:
<span class="math display">\[
\begin{aligned}
\bar{\mathbf{b}} &amp;= \mathbf{V}
\mathbf{X}^T\mathbf{y}/\sigma^2 \\
\mathbf{V} &amp;= \sigma^2(\mathbf{X}^T\mathbf{X} + \lambda
\mathbf{I})^{-1},
\end{aligned}
\]</span> such that <span class="math inline">\(\lambda =
\sigma^2/\sigma_0^2\)</span>, <span
class="math inline">\(\mathbf{I}\)</span> is the <span
class="math inline">\(p
\times p\)</span> identity matrix, and <span
class="math inline">\(\mathbf{X}\)</span> is the “input matrix”, that
is, the <span class="math inline">\(n \times p\)</span> matrix formed by
filling in each row <span class="math inline">\(i\)</span> with the
vector <span class="math inline">\(\mathbf{x}_i\)</span>. (Note that
because the posterior is multivariate normal, the posterior mean is also
the posterior mode.)</p>
<p>Since the posterior distribution is multivariate normal with analytic
expressions for the posterior mean and posterior covariance, this is a
case where Bayesian computational techniques such as MCMC or variational
inference are not strictly needed. However, if we are interested in
analyzing a large data set—large <span class="math inline">\(n\)</span>
and/or large <span class="math inline">\(p\)</span>—the computations
could be a problem. For example, consider the effort involved in
computing the matrix product <span class="math inline">\(\mathbf{X}^T
\mathbf{X}\)</span> and the matrix inverse that appears in the
expression for <span class="math inline">\(\mathbf{V}\)</span>. So
actually these techniques could be useful even if on paper the posterior
distribution is straightforward.</p>
</div>
</div>
<div id="simulating-the-ridge-regression-posterior-distribution"
class="section level2">
<h2>Simulating the ridge regression posterior distribution</h2>
<p>As our first attempt at grappling with the challenges of inference in
high dimensions, let’s consider a simple Gibbs sampler which involves
repeatedly choosing a dimension, <span class="math inline">\(j\)</span>,
and randomly sampling from the posterior distribution of <span
class="math inline">\(b_j\)</span> conditioned on all the other
dimensions: <span class="math display">\[
b_j \sim N(\mu_j, v_j^2),
\]</span> where <span class="math display">\[
\begin{aligned}
v_j &amp;= \bigg(\frac{\mathbf{x}_j^T\mathbf{x}_j}{s^2} +
            \frac{1}{s_0^2}\bigg)^{-1} \\
\mu_j &amp;= \frac{v_j}{s^2} \times
\bigg(\mathbf{x}_j^T\mathbf{y} -
\sum_{k \,\neq\, j} \mathbf{x}_j^T\mathbf{x}_k b_k \bigg).
\end{aligned}
\]</span></p>
<p><em>Discuss: What is the computational complexity of the Gibbs
sampler updates for ridge regression and how does it compare to the
computational complexity of the analytical posterior computations
above?</em></p>
<p>Let’s implement this Gibbs sampler and test it out on a moderately
large inference problem to gain some intuition for it, then we will draw
comparisons to the variational inference solution.</p>
</div>
<div id="the-data-set" class="section level2">
<h2>The data set</h2>
<p>Load the MASS package:</p>
<pre class="r"><code>library(MASS)</code></pre>
<p>And set the seed to ensure the results are reproducible.</p>
<pre class="r"><code>set.seed(3)</code></pre>
<p>We will use this function to simulate some data from a ridge
regression model:</p>
<pre class="r"><code># Simulate n data points from a ridge regression model with p inputs.
# Other parameters: p1, the number of nonzero coefficients to simulate
# (should not be greater than p); s, the residual standard deviation
# (s.d.); s0, the prior s.d. used to simulate the nonzero
# coefficients; and r, the correlation among the inputs.
sim_ridge_data &lt;- function (n, p, p1, s, s0, r) {
  R &lt;- matrix(r,p,p)
  diag(R) &lt;- 1
  X &lt;- mvrnorm(n,rep(0,p),R)
  X &lt;- scale(X,center = TRUE,scale = FALSE)
  b &lt;- rep(0,p)
  b[1:p1] &lt;- rnorm(p1,sd = s0)
  y &lt;- X %*% b + rnorm(n,sd = s)
  y &lt;- drop(scale(y,center = TRUE,scale = FALSE))
  return(list(X = X,y = y,b = b))
}</code></pre>
<p>Now simulate 80 data points from a ridge regression model with 24
inputs in which all but the first two inputs have coefficients of zero.
The 24 input variables are all quite strongly correlated with each other
(correlation of 0.8):</p>
<pre class="r"><code>n   &lt;- 80
p   &lt;- 24
s   &lt;- 0.6
s0  &lt;- 3
r   &lt;- 0.8
sim &lt;- sim_ridge_data(n,p,2,s,s0,r)
X   &lt;- sim$X
y   &lt;- sim$y</code></pre>
</div>
<div id="a-gibbs-sampler" class="section level2">
<h2>A Gibbs sampler</h2>
<p>This next bit of code defines a few functions used to implement the
Gibbs sampler, visualize the state of the Markov chain over time, and
compare to the analytical posterior distribution.</p>
<pre class="r"><code># Perform &quot;niter&quot; Gibbs sampling updates for each input variable in
# the ridge regression model with data X, y. The Markov chain is
# initialized to &quot;b&quot;.
ridge_gs &lt;- function (X, y, s, s0, niter, b = rep(0,ncol(X))) {
  p  &lt;- length(b)
  B  &lt;- matrix(0,p,niter)
  XX &lt;- crossprod(X)
  xy &lt;- drop(crossprod(X,y))
  for (i in 1:niter) {
    for (j in 1:p) {
      v    &lt;- 1/(XX[j,j]/s^2 + 1/s0^2)
      mu   &lt;- v * (xy[j] - sum(XX[j,-j]*b[-j]))/s^2
      b[j] &lt;- rnorm(1,mu,sqrt(v))
    }
    B[,i] &lt;- b
  }
  return(B)
}

# Return the posterior distribution for the ridge regression model
# given data X, y.
ridge_post &lt;- function (X, y, s, s0) {
  p &lt;- ncol(X)
  lambda &lt;- (s/s0)^2
  V &lt;- s^2 * solve(crossprod(X) + lambda*diag(p))
  b &lt;- drop(V %*% crossprod(X,y)/s^2)
  return(list(mean = b,var = diag(V)))
}

# This function is used to view how the parameter estimates change
# over time in the running of an inference algorithm (e.g.,
# MCMC). Input B is a p x niter matrix where p is the number of
# parameters and niter is the number of iterations performed.
# When show_average = TRUE, a running average is shown instead of
# the actual values in the B matrix.
plot_params_over_time &lt;- function (B, show_average = FALSE) {
  p     &lt;- nrow(B)
  niter &lt;- ncol(B)
  if (show_average)
    B[1,] &lt;- cumsum(B[1,])/1:niter
  plot(1:niter,B[1,],type = &quot;l&quot;,lwd = 1,col = &quot;dodgerblue&quot;,
       ylim = range(B),xlab = &quot;iteration&quot;,ylab = &quot;coefficient&quot;)
  for (j in 2:p) {
    if (show_average)
      B[j,] &lt;- cumsum(B[j,])/1:niter
    lines(1:niter,B[j,],lwd = 1,col = &quot;dodgerblue&quot;)
  }
}</code></pre>
<p>Let’s run the Gibbs sampler for 100 iterations:</p>
<pre class="r"><code>niter &lt;- 100
B &lt;- ridge_gs(X,y,s,s0,niter)</code></pre>
<p>Now plot the state of the Markov chain over time, and compare the
final state of the Markov chain to the exact posterior mean which we are
able to compute because the posterior is multivariate normal (the exact
means are the black dots in the plot):</p>
<pre class="r"><code>par(mar = c(4,4,1,0))
plot_params_over_time(B)
post &lt;- ridge_post(X,y,s,s0)
points(rep(niter,p),post$mean,pch = 20,col = &quot;darkblue&quot;,cex = 1)</code></pre>
<p><img src="figure/variational_inference.Rmd/plot-ridge-gs-1.png" width="288" style="display: block; margin: auto;" /></p>
<p>At the last iteration, the Markov chain is quite close to the exact
posterior mean. But we wouldn’t expect it to be exactly the same because
the MCMC is intended to simulate the full posterior distribution, not
just recover the posterior mean. However, if we instead take an average
of the Markov chain states, then we should (or hopefully) get closer to
the exact calculations. This next plot shows the running average across
the 100 Gibbs sampler iterations:</p>
<pre class="r"><code>par(mar = c(4,4,1,0))
plot_params_over_time(B,show_average = TRUE)
points(rep(niter,p),post$mean,pch = 20,col = &quot;darkblue&quot;,cex = 1)</code></pre>
<p><img src="figure/variational_inference.Rmd/plot-ridge-gs-running-average-1.png" width="288" style="display: block; margin: auto;" /></p>
</div>
<div id="a-different-iterative-algorithm" class="section level2">
<h2>A different iterative algorithm</h2>
<p>Let’s now consider a different iterative algorithm: on the one hand,
like the Gibbs sampler, this new algorithm updates one co-ordinate or
dimension at a time; on the other hand, unlike the Gibbs sampler, the
updates are <em>deterministic</em>. That is, given the same inputs, this
iterative algorithm will <em>always produce the same output.</em> To
start, we will simply describe this iterative algorithm, and later we
will motivate it as fitting a variational approxiation to the (exact)
posterior distribution.</p>
<div
id="posterior-distribution-for-the-single-input-ridge-regression-model"
class="section level3">
<h3>Posterior distribution for the “single-input” ridge regression
model</h3>
<p>To describe the algorithm, it will be helpful to first write down the
posterior distribution for a ridge regression model with a single input
(that is, <span class="math inline">\(p = 1\)</span>), which is a
special case of the expressions given above: <span
class="math display">\[
\bar{b} = \frac{v \mathbf{x}^T\mathbf{y}}{\sigma^2},
\qquad
v = \frac{\sigma^2}{\mathbf{x}^T\mathbf{x} + \lambda}.
\]</span> For describing the updates, it will be convenient to define
these expressions as functions of the data, so let’s change the notation
slightly: <span class="math display">\[
\bar{b}(\mathbf{x},\mathbf{y}) =
\frac{v(\mathbf{x}, \mathbf{y}) \, \mathbf{x}^T\mathbf{y}}{\sigma^2},
\qquad
v(\mathbf{x},\mathbf{y}) = \frac{\sigma^2}
{\mathbf{x}^T\mathbf{x} + \lambda}.
\]</span></p>
</div>
<div id="the-coordinatewise-updates" class="section level3">
<h3>The coordinatewise updates</h3>
<p>With these expressions, the coordinatewise updates for the iterative
algorithm involve two steps: <span class="math display">\[
\begin{aligned}
\mathbf{r}_j &amp;\leftarrow \mathbf{y} - \sum_{k\, \neq\, j}
\mathbf{x}_k b_k \\
b_j &amp;\leftarrow \bar{b}(\mathbf{x}_j, \mathbf{r}_j).
\end{aligned}
\]</span> In the machine learning literature, this update are often
viewed as “messages” sent between the coordinates and there is a lot of
work on studying these algorithms as “message-passing” algorithms.</p>
<p>Let’s now see what these updates look like on the example data
set.</p>
<p>The first function here computes the posterior mean and variance for
the single-input ridge regression model and the second function is
simply a pair of for-loops that repeatedly cycles through the
single-coordinate updates for all the <span
class="math inline">\(p\)</span> input variables.</p>
<pre class="r"><code># Return the posterior distribution for the single-input
# ridge regression model given data x, y.
ridge1_post &lt;- function (x, y, s, s0) {
  xx &lt;- sum(x^2)
  xy &lt;- sum(x*y)
  v &lt;- s^2/(xx + (s/s0)^2)
  b &lt;- v*xy/s^2
  return(list(mean = b,var = v))
}

# Perform &quot;niter&quot; updates of the iterative algorithm for ridge
# regression, initialized to &quot;b&quot;.
ridge_iterative &lt;- function (X, y, s, s0, niter, b = rep(0,ncol(X))) {
  p &lt;- length(b)
  B &lt;- matrix(0,p,niter)
  for (i in 1:niter) {
    r &lt;- drop(y - X %*% b)
    for (j in 1:p) {
      x &lt;- X[,j]
      r &lt;- r + x*b[j]
      b[j] &lt;- ridge1_post(x,r,s,s0)$mean
      r &lt;- r - x*b[j]
    }
    B[,i] &lt;- b
  }
  return(B)
}</code></pre>
<p>Let’s now run 20 rounds of this updates and look at how the
coefficients <span class="math inline">\(b_j\)</span> change over
time:</p>
<pre class="r"><code>par(mar = c(4,4,1,0))
niter &lt;- 20
B &lt;- ridge_iterative(X,y,s,s0,niter)
plot_params_over_time(B)
points(rep(niter,p),post$mean,pch = 20,col = &quot;darkblue&quot;,cex = 1)</code></pre>
<p><img src="figure/variational_inference.Rmd/plot-ridge-iterative-1.png" width="288" style="display: block; margin: auto;" /></p>
<p>Notice that the iterative estimates of the coefficients get
progressively closer to the analytical posterior mean. What happens if
you run the iterative algorithm longer? Does it ever recover the
analytical solution? Do the iterates eventually stop changing (that is,
do they “converge” to a fixed point)? Here, we started at an initial
estimate where all the coefficients were zero. What happens if we give
the algorithm a random starting point?</p>
<p><em>Discuss differences between this iterative algorithm and the
Gibbs sampler. In particular, compare the computational complexity of
this iterative algorithm to the Gibbs sampler</em></p>
</div>
</div>
<div id="posterior-inference-as-optimization" class="section level2">
<h2>Posterior inference as optimization</h2>
<p>Posterior inference is fundamentally an integration problem; that is,
computing posterior distributions and expectations with respect to the
posterior distribution always involves sums or integrals (and sometimes
these integrals have known closed-form formulas). The “magic” of
variational inference is that it recasts this integration problem as an
optimization problem. Here we will see how that happens generally, and
for the ridge regression model. The key question we will answer is:
<em>what is the objective function we are optimizing?</em></p>
<p><strong>Note:</strong> One possible point of confusion with the ridge
expression model is that the posterior mean is also the posterior mode,
and as a result the iterative algorithm can also be viewed as an
algorithm for finding the posterior mode (i.e., the <em>maximum a
posteriori</em> estimate). We will try to avoid this perspective here
since our goal is to to illustrate variational inference ideas that
could be applicable to other models, not just ridge regression.</p>
<div id="the-kullback-leibler-divergence" class="section level3">
<h3>The Kullback-Leibler divergence</h3>
<p>Let <span class="math inline">\(q(\mathbf{b})\)</span> denote an
approximation to the true posterior <span
class="math inline">\(p_{\mathrm{post}}(\mathbf{b}) = p(\mathbf{b} \mid
\mathbf{X}, \mathbf{y})\)</span>. The starting point of all variational
inference methods is a measure of the difference between the true
posterior and the approximation. Here (like most variational inference
approaches) we will us the Kullback-Leibler (K-L) divergence measure.
The K-L divergence between <span
class="math inline">\(q(\mathbf{b})\)</span> and the posterior is is
<span class="math display">\[
\mathrm{KL}(q \,\|\, p_{\mathrm{post}}) =
\int q(\mathbf{b})
\log \bigg\{\frac{q(\mathbf{b})}{p_{\mathrm{post}}(\mathbf{b})}\bigg\}
\,
d\mathbf{b}.
\]</span> (It is called a “divergence” instead of a “difference” to
remind us that this is not a symmetric measure since differences are
usually symmetric.)</p>
<p>Smaller “divergences” mean that the two distributions are more
similar, and when they are the same, the K-L divergence is zero.</p>
<p><em>Intuitively, our goal is to find a <span
class="math inline">\(q\)</span> that makes the K-L divergence as small
as possible. Therefore, the K-L divergence is the objective we are
optimizing (although in practice it isn’t exactly the K-L
divergence).</em></p>
<p>Expanding the terms in the K-L divergence using properties of the
logarithm, and expanding out the posterior (Bayes’ Theorem), the K-L
divergence works out to <span class="math display">\[
\mathrm{KL}(q \,\|\, p_{\mathrm{post}}) =
F(q) + \log Z,
\]</span> where <span class="math display">\[
\begin{aligned}
F(q) &amp;= U(q) - H(q) \\
H(q) &amp;= - \textstyle \int q(\mathbf{b}) \log q(\mathbf{b}) \,
d\mathbf{b} \\
U(q) &amp;= - \textstyle \int q(\mathbf{b}) \log p(\mathbf{b}) \,
d\mathbf{b} \\
Z &amp;= \textstyle \int p(\mathbf{y} \mid \mathbf{X}, \mathbf{b}) \,
p(\mathbf{b}) \, d\mathbf{b}.
\end{aligned}
\]</span> In statistical physics, <span
class="math inline">\(H(q)\)</span> is known as the “entropy”, <span
class="math inline">\(U(q)\)</span> is the “variational average energy”
and <span class="math inline">\(F(q)\)</span> is the “variational free
energy”.</p>
<p><em>Exercise: Derive this result.</em></p>
<p>Luckily, although <span class="math inline">\(\log Z\)</span> has an
ugly and potentially difficult-to-compute integral inside of it, it does
not depend on <span class="math inline">\(q\)</span>, so we can ignore
it! In other words, we can equivallently minimize <span
class="math inline">\(F(q)\)</span> (the variational free energy)
instead of the K-L divergence. The convention in machine learning is
instead to maximize <span class="math inline">\(-F(q)\)</span>, which is
called the “Evidence Lower Bound”, or “ELBO”: <span
class="math display">\[
\mathrm{ELBO}(q) =
E_q[\log p(\mathbf{b})] - E_q[\log q(\mathbf{b})].
\]</span> Notice that I’ve rewritten the integrals as expectations.
<em>So now are goal is to find a <span class="math inline">\(q\)</span>
that maximizes the ELBO.</em></p>
<p><em>Optional exercise: Rewrite all these expressions using
expectations instead of integrals.</em></p>
<p><em>When <span class="math inline">\(q(\mathbf{b}) =
p_{\mathrm{post}}(\mathbf{b})\)</span>, what is the ELBO equal
to?</em></p>
</div>
<div id="a-fully-factorized-variational-approximation"
class="section level3">
<h3>A fully-factorized variational approximation</h3>
<p>Recall our goal is to be able to tackle large-scale inference
problems. This inevitably requires some sort of compromise: we will have
to accept some inaccuracy in the result to achieve faster computations.
In variational inference, one can either approximate the ELBO itself, or
restrict <span class="math inline">\(q\)</span> to a distribution that
is particularly convenient computationally. One such constraint is that
<span class="math inline">\(q\)</span> factorize into its individual
coordinates; that is, <span class="math display">\[
q(\mathbf{b}) = \prod_{j=1}^p q_j(b_j).
\]</span> In statistical physics, this is called the “mean field”
approximation, and is actually an old idea dating back to the 1940s and
1950s. The machine learning community has adopted much of the
terminology from statistical physics and so also calls these “mean
field” approximations.</p>
<p>Clearly, this approximation will be accurate if the true posterior
distribution also factorizes in this way. (<em>Question: when does the
ridge regression posterior factorize in this way?</em>) On the other
hand, this approximation can be very poor if there are strong
correlations among the coordinates. But keep in mind that poor
approximations can still (sometimes) be useful!</p>
<p>For some intuition why this approximation is helpful, the hope is
that with the property of <span class="math inline">\(q\)</span> being
fully-factorized, the <span class="math inline">\(p\)</span>-dimensional
integrals over <span class="math inline">\(\mathbf{b}\)</span> will
decompose into much lower dimensional integrals, say, in 1 or 2
dimensions, which makes the problem much more tractable. <em>For
example, with a fully-factorized <span class="math inline">\(q\)</span>,
how does the expectation/integral</em> <span
class="math inline">\(E_q[\mathbf{X} \mathbf{b}]\)</span> <em>decompose
into smaller integrals? What is the dimension of those
integrals?</em></p>
</div>
<div id="divide-and-conquer" class="section level3">
<h3>Divide and conquer</h3>
<p>In summary, our optimization problem is to maximize the ELBO with the
constraint that <span class="math inline">\(q\)</span> is
fully-factorized. However it isn’t yet clear how this connects to our
iterative algorithm above. The connection is made by taking a “divide
and conquer” approach: instead of trying to optimize the entire <span
class="math inline">\(q(\mathbf{b})\)</span> at once, we instead
optimize a single coordinate at a to,e, <span
class="math inline">\(q_j(b_j)\)</span>. <em>Doing this will lead to the
updates given above.</em> This is a very widely used strategy in
variational inference and is The proof is a bit tedious and too long to
explain in detail here, but it isn’t hard to give a high-level
explanation.</p>
<p><em>Proof sketch:</em></p>
<ol style="list-style-type: decimal">
<li><p>Work out the expression for the ELBO for the single-input ridge
regression model, that is, for the special case when <span
class="math inline">\(p = 1\)</span>. Denote this by <span
class="math inline">\(\mathrm{ELBO}^{(p=1)}(q; \mathbf{x},
\mathbf{y})\)</span> to make the dependence of the ELBO on the data
explicit.</p></li>
<li><p>Next consider the more general (<span class="math inline">\(p
&gt; 1\)</span>) case, but expand only the terms in the ELBO involving
<span class="math inline">\(q_j\)</span>. If done carefully, this should
give the following result: <span class="math display">\[
\mathrm{ELBO}(q) =
\mathrm{ELBO}^{(p=1)}(q_j; \mathbf{x}_j, \mathbf{r}_j) +
\mathrm{const},
\]</span> in which <span class="math inline">\(\mathbf{r}_j\)</span> was
defined above, and the “const” includes all the terms that do not
involve <span class="math inline">\(q_j\)</span>.</p></li>
</ol>
<p>In other words, the ELBO can be rearranged to exactly match the
expression for the single-input ELBO if we ignore terms not involving
<span class="math inline">\(q_j\)</span>. This means that optimizing one
coordinate at a time reduces to computing the posterior distribution for
a single-input ridge regression model, which is much more manageable
than computing the posterior distribution for a ridge regression model
with, say, thousands of inputs.</p>
<p><em>The full derivation of this result is left as an
exercise.</em></p>
<p>By deriving the coordinatewise updates in this way, we have
accomplished several things, including:</p>
<ol style="list-style-type: decimal">
<li><p>We can understand the algorithm as an optimization algorithm that
is optimizing a specific objective function (the ELBO).</p></li>
<li><p>The updates, when they “converge” (stop changing), should
(usually) recover a maximum of the ELBO.</p></li>
<li><p>We can understand this algorithm as making the approximation that
the posterior is fully-factorized. (It turns out that for ridge
regression, the posterior means are always exact under this
approximation, which is why we saw above that these updates produced
coefficients that were very close to the exact posterior means. The
variances, on the other hand, which we did not keep track of, do not
recover the exact posterior variances.</p></li>
<li><p>We can also use the ELBO to monitor progress of the
updates.</p></li>
</ol>
</div>
</div>
<div id="connection-to-em" class="section level2">
<h2>Connection to EM</h2>
<p>Finally, note that the close resemblence of the ELBO to <span
class="math inline">\(F(\theta, q)\)</span> in <a
href="em_variational.html">the vignette on EM</a> is not a coincidence.
In fact, this connection has been exploited to develop approximate EM
algorithms sometimes called “variational EM”.</p>
</div>
<div id="further-reading" class="section level2">
<h2>Further reading</h2>
<ul>
<li><p>Blei et al, <a
href="https://doi.org/10.1080/01621459.2017.1285773">Variational
inference: a review for statisticians.</a></p></li>
<li><p>For a statistical physics perspective, see Yedidia et al, <a
href="https://doi.org/10.1109/TIT.2005.850085">Constructing free-energy
approximations and generalized belief propagation
algorithms.</a></p></li>
</ul>
<br>
<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-sessioninfo" data-toggle="collapse" data-target="#workflowr-sessioninfo" style="display: block;">
<span class="glyphicon glyphicon-wrench" aria-hidden="true"></span>
Session information
</button>
</p>
<div id="workflowr-sessioninfo" class="collapse">
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>R version 4.3.3 (2024-02-29)
Platform: aarch64-apple-darwin20 (64-bit)
Running under: macOS Sonoma 14.7.1

Matrix products: default
BLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib 
LAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

time zone: America/Chicago
tzcode source: internal

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] MASS_7.3-60.0.1

loaded via a namespace (and not attached):
 [1] vctrs_0.6.5       cli_3.6.4         knitr_1.45        rlang_1.1.5      
 [5] xfun_0.42         highr_0.10        stringi_1.8.3     promises_1.2.1   
 [9] jsonlite_1.8.8    workflowr_1.7.1   glue_1.8.0        rprojroot_2.0.4  
[13] git2r_0.33.0      htmltools_0.5.8.1 httpuv_1.6.14     sass_0.4.9       
[17] fansi_1.0.6       rmarkdown_2.26    evaluate_0.23     jquerylib_0.1.4  
[21] tibble_3.2.1      fastmap_1.1.1     yaml_2.3.8        lifecycle_1.0.4  
[25] whisker_0.4.1     stringr_1.5.1     compiler_4.3.3    fs_1.6.5         
[29] Rcpp_1.0.12       pkgconfig_2.0.3   later_1.3.2       digest_0.6.34    
[33] R6_2.5.1          utf8_1.2.4        pillar_1.9.0      magrittr_2.0.3   
[37] bslib_0.6.1       tools_4.3.3       cachem_1.0.8     </code></pre>
</div>
</div>

<hr>
<p>
    This site was created with <a href="http://rmarkdown.rstudio.com">R Markdown</a>
</p>
<hr>

<!-- To enable disqus, uncomment the section below and provide your disqus_shortname -->

<!-- disqus
  <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'rmarkdown'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
-->

<!-- Adjust MathJax settings so that all math formulae are shown using
TeX fonts only; see
https://docs.mathjax.org/en/latest/web/configuration.html. This will make
the presentation more consistent at the cost of the webpage sometimes
taking slightly longer to load. Note that this only works because the
footer is added to webpages before the MathJax javascript. -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>




</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
